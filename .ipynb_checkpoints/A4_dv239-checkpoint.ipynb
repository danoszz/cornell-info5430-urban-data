{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46e61e6",
   "metadata": {
    "id": "bizarre-tonight"
   },
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d1aaa",
   "metadata": {
    "id": "patent-china"
   },
   "source": [
    "#### In this assignment, we will conclude our analysis of whether the stop and frisk policy was racially discriminatory, but from a very different angle than our previous mapping analysis. We will rely heavily on logistic regression and regularized regression, as discussed in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f9be8",
   "metadata": {
    "id": "lPy-JHVGfPsF"
   },
   "source": [
    "# Data processing (5 points)\n",
    "\n",
    "First we need to do some data processing for consistency with previous of analysis of stop_and_frisk data. Load in the data \"sqf_sample\" and filter for stops between 2008 and 2012 (including both 2008 and 2012 in your sample). Filter for stops of white, Black, and Hispanic pedestrians using the suspect_race column. (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0bcc4",
   "metadata": {
    "id": "Kd54AVi-gUOq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51f7a42",
   "metadata": {
    "id": "ranging-edition"
   },
   "source": [
    "## Using regression to analyze the frisk decision. (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec2b8a",
   "metadata": {
    "id": "canadian-signature"
   },
   "source": [
    "We will start by testing for racial discrimination in the decision to conduct a frisk after a stop: ie, whether Black and Hispanic pedestrians are more likely to be frisked (patted down for weapons) after they are stopped, controlling for other factors.\n",
    "\n",
    "a. Using statsmodels, perform a logistic regression, using `frisked` as the dependent variable and `suspect_race` as the independent variable, to assess how the probability of being frisked after a stop varies by race. Write a few sentences interpreting the results, making sure to answer the following questions: which value of suspect_race is omitted from the regression coefficients? Which race groups are most likely to be frisked after being stopped? How do you interpret the magnitude and sign of the coefficients? How do you interpret their statistical significance and confidence intervals? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103dd3ae",
   "metadata": {
    "id": "qx5cT_p0gWE8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c9dee0",
   "metadata": {
    "id": "m8WYaqSvWoRI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e015b763",
   "metadata": {
    "id": "earned-mainstream"
   },
   "source": [
    "b. Now perform a linear regression instead of a logistic regression using the same formula. How is the interpretation of the coefficients similar or different in the two regressions? What are the advantages of each? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbce0c1",
   "metadata": {
    "id": "1lTNBfBqgXMW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe5b619f",
   "metadata": {
    "id": "rgk3tioqWpJg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4603dfee",
   "metadata": {
    "id": "constitutional-place"
   },
   "source": [
    "c. The regression using only race as an independent variable is a good starting point, but it does not control for any other variables. What other variables do you think are important to control for, and why?  (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0be34",
   "metadata": {
    "id": "pS3NmINSWsDL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "777aac25",
   "metadata": {
    "id": "assumed-employment"
   },
   "source": [
    "d. Run a logistic regression where you control for both race and for the \"precinct\" variable, which encodes the police precinct in which the stop occurred. Make sure to control for precinct as a categorical, not a numerical, variable, by writing it as C(precinct) in the regression formula - why is this important to do?\n",
    "\n",
    "How do the race coefficients change, and what does that mean? How does the interpretation of this regression differ from the regression in which you only control for race? Make one argument in favor of reporting results controlling for location, and one argument against it. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c061669",
   "metadata": {
    "id": "dI-I4F9jgYqr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f79e85a6",
   "metadata": {
    "id": "xddOFV6kWvzy"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2461e0b",
   "metadata": {
    "id": "expected-whole"
   },
   "source": [
    "e. In a few sentences, explain why it is a BAD idea, conceptually, to control for the following variables if we are trying to assess whether the police racially discriminate in whom they frisk after a stop: a) \"found.weapon\", which encodes whether the frisk found a weapon, and b) \"suspect.eye\" and \"suspect.hair\", which encode the suspect's eye and hair color. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a13e3",
   "metadata": {
    "id": "_5dWS0LAW6De"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb11335",
   "metadata": {
    "id": "closed-difference"
   },
   "source": [
    "f. In a few sentences, explain the problem of omitted variable bias in this analysis, and how it would undermine the conclusions. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f1143",
   "metadata": {
    "id": "74cAqUPgW7Mu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd95f71a",
   "metadata": {
    "id": "mBwIUjuRt5o2"
   },
   "source": [
    "g. In a few sentences, explain why only examining whether someone is frisked after a stop might fail to provide a full picture of discrimination in the stop and frisk policy. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f772e7",
   "metadata": {
    "id": "IuBEAzCVW7zQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ae6b114",
   "metadata": {
    "id": "grateful-chassis"
   },
   "source": [
    "## Outcome analysis using regularized regression (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfa2d7",
   "metadata": {
    "id": "scientific-franchise"
   },
   "source": [
    "Because of the issues with omitted variables in analyses like the one above, *outcome* tests are often used: these look not at the rate at which a decision is made (like the decision to frisk), but at the outcome of the decision (for example, if the frisk is conducted to find a weapon, does it actually find one?) Now we will use an outcome-style analysis. Specifically, we will fit a machine learning model to predict the probability that each stop which was conducted on suspicion the pedestrian possessed a weapon actually finds a weapon. Stops which are very unlikely to find a weapon arguably violate the Fourth Amendment, which prohibits unreasonable searches; if such stops disproportionately occur of certain race groups, the policy may violate the Fourteenth Amendment, which prohibits racial discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52db6b1",
   "metadata": {
    "id": "-CaMrTP0NlPY"
   },
   "source": [
    "a. In this portion of this analysis, you will be using a smaller version of the data to speed up model fitting. Load in \"small_sqf_sample\". As before, we need to do some data processing for consistency with previous of analysis of this data. Load in the data and filter for stops between 2008 and 2012 (including both 2008 and 2012); filter for stops of white, Black, and Hispanic pedestrians using the suspect_race column; and filter for stops conducted on suspicion of criminal posession of a weapon (ie, suspected_crime == 'cpw'). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28026bbb",
   "metadata": {
    "id": "cDZV0_2ygaOM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a482b8f7",
   "metadata": {
    "id": "confident-array"
   },
   "source": [
    "\n",
    "b. We will be fitting the regression model \n",
    "\n",
    "`found_weapon ~ C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex + ADDITIONAL_CIRCUMSTANCE_COLUMNS` \n",
    "\n",
    "where ADDITIONAL_CIRCUMSTANCE_COLUMNS are any columns that begin with \"stopped_bc\" or \"additional_\" besides \"additional_other\" and \"stopped_bc_other\". You can get these columns by running\n",
    "\n",
    "`ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in d.columns if ('stopped_bc' in a or 'additional_' in a) and a not in (['additional_other', 'stopped_bc_other'])]`\n",
    "\n",
    "You should have 18 additional columns. These columns provide more information about the circumstances of the stop, and we include them for consistency with the original analysis and because they turn out to be important for predictive performance. \n",
    "\n",
    "Drop any rows with missing values in any of the variables you need. \n",
    "\n",
    "Now, we need to put the data into a format which sklearn can use later - ie, numpy arrays. Do this with \"patsy\" library and the \"dmatrix\" function. You can call dmatrix as follows:\n",
    "\n",
    "`sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS),sqf_data, return_type='dataframe')`\n",
    "\n",
    "and it will return a dataframe on which you can fit a regression model. The first argument to dmatrix is the formula that you want to use to make the dataframe; the second argument gives patsy the data you want to use; return_type='dataframe' ensures that you get a dataframe, not a patsy object which is hard to use.\n",
    "\n",
    "Look at the output of dmatrix and explain what the columns mean. Why can't we just pass the columns from the original dataframe directly into the sklearn function? (8 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d2e8d",
   "metadata": {
    "id": "n42o63nLgdET"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05330447",
   "metadata": {
    "id": "N3N-RXXKX1PQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b9ad80",
   "metadata": {
    "id": "AAib9e-WfUdF"
   },
   "source": [
    "c. As discussed in class, when fitting machine learning models, you should always divide the dataset into a train, val, and test set. Randomly divide the filtered, processed data into three pieces - the train set (60%), the val set (20%) and the test set (20%). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c8a11",
   "metadata": {
    "id": "ditQALf_geMN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73410d7",
   "metadata": {
    "id": "MWZp2Jw7TKVm"
   },
   "source": [
    "d. We will be training a regularized logistic regression model to predict the outcome. When using many machine learning models, including regularized logistic regression, it is important to preprocess the input features so they are all on the same scale, for reasons discussed in class. In this case, we will take each column in the input data, subtract its mean, and divide by its standard deviation. This makes it so each column of the data has mean 0 and standard deviation 1. \n",
    "\n",
    "When scaling the data, it is important to compute the scaling using only the train set, as shown in class. The reason is that we are pretending that the train data is all we have access to to fit our model fitting pipeline, so we cannot \"peek\" at the validation or test sets to generate our scaling. Use sklearn's StandardScaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to fit a scaling transform on the train set and transform the train set (you can use the fit_transform method on the train set).Then apply the fitted StandardScaler model to the validation and test sets as well (using the transform --- not the fit --- method). (Look at the notebook we went through in class on regularization and lasso if you are confused.) The transformed datasets are the final datasets you will feed into your logistic regression model. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a20602",
   "metadata": {
    "id": "FyVKwlbjgffV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d2149c",
   "metadata": {
    "id": "involved-implement"
   },
   "source": [
    "e. Uisng sklearn.linear_model.LogisticRegression, fit a model on the train set to predict found_weapon. Set the \"penalty\" argument to \"none\" so that the model will not use any regularization; this corresponds to fitting a regular logistic regression model. If you get a `ConvergenceWarning`, increase the number of iterations using the `max_iter` argument; this means the model optimization needs more iterations to converge. (Look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) if you are uncertain which arguments to use!)\n",
    "\n",
    "A standard measure of predictive performance for binary outcome variables like found_weapon is AUC. Higher values of AUC are better; an AUC of 1 means that the model is perfectly predicting the outcome; an AUC of 0.5 means that it is predicting it only as well as random chance. Report the AUC of the fitted model (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) on both the train and validation sets. What is the problem with using accuracy as a metric for this task? How does the train set AUC differ from the validation AUC, and does this make sense? (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0497a7e",
   "metadata": {
    "id": "ExHdN87ggimI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e531c6",
   "metadata": {
    "id": "1Ep5iHbBhn1z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20ffede",
   "metadata": {
    "id": "eight-absorption"
   },
   "source": [
    "f. Our logistic regression model is not using any regularization and appears to be overfitting. We will try to use regularization to reduce overfitting. You will be fitting an L1-penalized logistic regression model using code that looks something like\n",
    "\n",
    "`LogisticRegression(C=sparsity_param, penalty='l1', solver='liblinear')`\n",
    "\n",
    "(the \"l1\" specifies that we're using an L1 penalty, as discussed in class; the \"liblinear\" solver is an optimizer that works with the L1 penalty. See the logistic regression documentation for more details.) \n",
    "\n",
    "Increase and decrease the amount of regularization using different values of the C parameter, searching logarithmically over at least 20 values in the range from 1e-2 to 1. (Note: we do not recommend defining a variable named \"C\" in your code, because this may cause weird patsy issues, since patsy also uses \"C\". Give the variable another name. Sorry! I complained to the patsy people.) \n",
    "\n",
    "Print out the train set, val set, and test set AUC for each value of of the regularization parameter. Make a plot where the x-axis is the regularization parameter and the y-axis is AUC, with one line for train AUC, one line for val AUC, and one line for test AUC (use plt.semilogx to plot the lines so the x-axis will be logarithmic, making it easier to see the plot). Comment on the trends. Do you see evidence of overfitting? Explain. For the rest of this assignment, use the model with the highest AUC on the validation set. (10 points)\n",
    "\n",
    "In a full analysis, it would make sense to play with other aspects of the model as well: for example, you could try using other forms of regularization (like L1 vs L2) or other classification algorithms besides logistic regression. The basic pattern, though, would be the same: fit the model on the train set, choose models on the val set, and once you've chosen your best model, assess your results (once!) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264a089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d19e6ac",
   "metadata": {
    "id": "owMXSAmTeInj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "569fe9c3",
   "metadata": {
    "id": "antique-connection"
   },
   "source": [
    "g. Assess model *calibration* on the test set. This checks whether the model's predicted probabilities line up with the true probabilities, and is important to assess here because we will be analyzing the model's predicted probabilities. To assess calibration, take the 10% of rows of the test set with the highest model predictions and compare the mean model predicted probability (ie, the output of `predict_proba`) to the actual mean of the outcome variable. Repeat for the next 10% of rows, and for all 10% groups. Make a plot comparing the model predicted probabilities on each 10% group to the actual outcomes (mean predicted probability on the x-axis and mean actual outcome on the y-axis). You should end up with a plot with 10 points, one for each 10% group. Plot the line y=x so you can see how well the predicted probabilities line up with the actual probabilities - ideally, your points will lie on the line or close to it. (10 points)\n",
    "\n",
    "Calibration is an issue for many machine learning models, including deep learning models, so this is always a good thing to check. In a full analysis, it would make sense to check calibration (and AUC) for subgroups as well (eg, each race group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4511d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb2f5ca",
   "metadata": {
    "id": "frozen-apollo"
   },
   "source": [
    "h. Using the test set, compute the fraction of observations which have lower than a 2% model-predicted probability of finding a weapon for Black, Hispanic, and white pedestrians. Stops below this threshold are extremely unlikely to have resulted in finding a weapon, arguably violating the Fourth Amendment. Repeat this for 20 thresholds evenly spaced between 1% and 5%. Make a graph where the x-axis is the threshold, and the y-axis is the fraction of stops falling below that threshold, with one line for each race group. Explain what you observe. Do you think this graph provides evidence for Fourteenth Amendment violations (racial discrimination)? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572f8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37a54edf",
   "metadata": {
    "id": "organized-luxembourg"
   },
   "source": [
    "i. The analysis you have performed in the second part of this assignment is very similar to the analysis in [this paper](https://5harad.com/papers/stop-and-frisk.pdf). Read the paper and write a few sentences about their main conclusions. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516625e",
   "metadata": {
    "id": "6wMU2Jzse3Dm"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
