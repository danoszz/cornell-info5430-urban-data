{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bizarre-tonight"
   },
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bizarre-tonight"
   },
   "source": [
    "Isaiah D. Murray (idm22@cornell.edu)\n",
    "\n",
    "Daan van der Zwagg (dv239@cornell.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "patent-china"
   },
   "source": [
    "#### In this assignment, we will conclude our analysis of whether the stop and frisk policy was racially discriminatory, but from a very different angle than our previous mapping analysis. We will rely heavily on logistic regression and regularized regression, as discussed in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPy-JHVGfPsF"
   },
   "source": [
    "# Data processing (5 points)\n",
    "\n",
    "First we need to do some data processing for consistency with previous of analysis of stop_and_frisk data. Load in the data \"sqf_sample\" and filter for stops between 2008 and 2012 (including both 2008 and 2012 in your sample). Filter for stops of white, Black, and Hispanic pedestrians using the suspect_race column. (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kd54AVi-gUOq"
   },
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv\n",
    "sqf_sample = pd.read_csv(\"sqf_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqf_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408615, 89)\n"
     ]
    }
   ],
   "source": [
    "# check shape\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'date', 'time', 'precinct', 'xcoord', 'ycoord', 'serial',\n",
      "       'radio_run', 'inside_outside', 'location_housing', 'observation_period',\n",
      "       'suspected_crime', 'stop_length', 'identification', 'reason_explained',\n",
      "       'others_stopped', 'arrested', 'arrested_reason', 'summons_issued',\n",
      "       'officer_uniform', 'officer_verbal', 'officer_shield', 'frisked',\n",
      "       'searched', 'extra_reports', 'force_hands', 'force_wall',\n",
      "       'force_ground', 'force_drawn', 'force_pointed', 'force_baton',\n",
      "       'force_handcuffs', 'force_pepper', 'force_other', 'stopped_bc_object',\n",
      "       'stopped_bc_desc', 'stopped_bc_casing', 'stopped_bc_lookout',\n",
      "       'stopped_bc_clothing', 'stopped_bc_drugs', 'stopped_bc_furtive',\n",
      "       'stopped_bc_violent', 'stopped_bc_bulge', 'stopped_bc_other',\n",
      "       'frisked_bc_suspected_crime', 'frisked_bc_weapons', 'frisked_bc_attire',\n",
      "       'frisked_bc_actual_crime', 'frisked_bc_noncompliance',\n",
      "       'frisked_bc_threats', 'frisked_bc_prior', 'frisked_bc_furtive',\n",
      "       'frisked_bc_bulge', 'additional_report', 'additional_investigation',\n",
      "       'additional_proximity', 'additional_evasive', 'additional_associating',\n",
      "       'additional_direction', 'additional_highcrime', 'additional_time',\n",
      "       'additional_sights', 'additional_other', 'searched_hardobject',\n",
      "       'searched_outline', 'searched_admission', 'searched_other',\n",
      "       'found_contraband', 'found_pistol', 'found_rifle', 'found_assault',\n",
      "       'found_knife', 'found_machinegun', 'found_other', 'suspect_sex',\n",
      "       'suspect_race', 'suspect_hispanic', 'suspect_age', 'suspect_dob',\n",
      "       'suspect_height', 'suspect_weight', 'suspect_hair', 'suspect_eye',\n",
      "       'suspect_build', 'found_gun', 'found_weapon', 'id', 'lat', 'lon'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>precinct</th>\n",
       "      <th>xcoord</th>\n",
       "      <th>ycoord</th>\n",
       "      <th>serial</th>\n",
       "      <th>radio_run</th>\n",
       "      <th>inside_outside</th>\n",
       "      <th>location_housing</th>\n",
       "      <th>...</th>\n",
       "      <th>suspect_height</th>\n",
       "      <th>suspect_weight</th>\n",
       "      <th>suspect_hair</th>\n",
       "      <th>suspect_eye</th>\n",
       "      <th>suspect_build</th>\n",
       "      <th>found_gun</th>\n",
       "      <th>found_weapon</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009-02-13</td>\n",
       "      <td>13:01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>990071.0</td>\n",
       "      <td>204546.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>185.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1603997</td>\n",
       "      <td>40.728106</td>\n",
       "      <td>-73.978998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-02</td>\n",
       "      <td>22:11</td>\n",
       "      <td>77.0</td>\n",
       "      <td>996834.0</td>\n",
       "      <td>184061.0</td>\n",
       "      <td>6369.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>175.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2422905</td>\n",
       "      <td>40.671873</td>\n",
       "      <td>-73.954636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>2007-05-18</td>\n",
       "      <td>23:28</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1002197.0</td>\n",
       "      <td>182674.0</td>\n",
       "      <td>2053.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>175.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>705704</td>\n",
       "      <td>40.668056</td>\n",
       "      <td>-73.935306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-01-20</td>\n",
       "      <td>19:20</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1010719.0</td>\n",
       "      <td>186857.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>transit</td>\n",
       "      <td>...</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>180.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>heavy</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2133381</td>\n",
       "      <td>40.679516</td>\n",
       "      <td>-73.904570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>19:50</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1014014.0</td>\n",
       "      <td>249325.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>175.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3425345</td>\n",
       "      <td>40.850964</td>\n",
       "      <td>-73.892414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        date   time  precinct     xcoord    ycoord  serial  radio_run  \\\n",
       "0  2009  2009-02-13  13:01       9.0   990071.0  204546.0   730.0      False   \n",
       "1  2010  2010-07-02  22:11      77.0   996834.0  184061.0  6369.0      False   \n",
       "2  2007  2007-05-18  23:28      71.0  1002197.0  182674.0  2053.0      False   \n",
       "3  2010  2010-01-20  19:20      73.0  1010719.0  186857.0   967.0      False   \n",
       "4  2012  2012-01-18  19:50      48.0  1014014.0  249325.0   334.0      False   \n",
       "\n",
       "   inside_outside location_housing  ...  suspect_height suspect_weight  \\\n",
       "0           False          neither  ...        6.166667          185.0   \n",
       "1           False          neither  ...        5.666667          175.0   \n",
       "2            True          neither  ...        5.750000          175.0   \n",
       "3            True          transit  ...        5.666667          180.0   \n",
       "4           False          neither  ...        6.000000          175.0   \n",
       "\n",
       "   suspect_hair suspect_eye  suspect_build  found_gun  found_weapon       id  \\\n",
       "0         black       brown         medium      False         False  1603997   \n",
       "1         black       brown         medium      False         False  2422905   \n",
       "2         black       brown         medium      False         False   705704   \n",
       "3         black       brown          heavy      False         False  2133381   \n",
       "4         black       brown         medium      False         False  3425345   \n",
       "\n",
       "         lat        lon  \n",
       "0  40.728106 -73.978998  \n",
       "1  40.671873 -73.954636  \n",
       "2  40.668056 -73.935306  \n",
       "3  40.679516 -73.904570  \n",
       "4  40.850964 -73.892414  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for year\n",
    "data = data[data[\"year\"]>=2008]\n",
    "data = data[data[\"year\"]<=2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293278, 89)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the data shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2009 2010 2012 2008 2011]\n"
     ]
    }
   ],
   "source": [
    "# making sure that there are in fact only years 2008 to 2012\n",
    "print (data[\"year\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black' 'hispanic' 'asian' 'white' nan 'other' 'native american']\n"
     ]
    }
   ],
   "source": [
    "# seeing the unique values in suspect_race\n",
    "print (data[\"suspect_race\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for race\n",
    "selected = ['black', 'hispanic', 'white']\n",
    "data = data[data[\"suspect_race\"].isin(selected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black' 'hispanic' 'white']\n"
     ]
    }
   ],
   "source": [
    "# seeing the unique values in suspect_race\n",
    "print (data[\"suspect_race\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273235, 89)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>precinct</th>\n",
       "      <th>xcoord</th>\n",
       "      <th>ycoord</th>\n",
       "      <th>serial</th>\n",
       "      <th>radio_run</th>\n",
       "      <th>inside_outside</th>\n",
       "      <th>location_housing</th>\n",
       "      <th>...</th>\n",
       "      <th>suspect_height</th>\n",
       "      <th>suspect_weight</th>\n",
       "      <th>suspect_hair</th>\n",
       "      <th>suspect_eye</th>\n",
       "      <th>suspect_build</th>\n",
       "      <th>found_gun</th>\n",
       "      <th>found_weapon</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009-02-13</td>\n",
       "      <td>13:01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>990071.0</td>\n",
       "      <td>204546.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>185.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1603997</td>\n",
       "      <td>40.728106</td>\n",
       "      <td>-73.978998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-02</td>\n",
       "      <td>22:11</td>\n",
       "      <td>77.0</td>\n",
       "      <td>996834.0</td>\n",
       "      <td>184061.0</td>\n",
       "      <td>6369.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>175.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2422905</td>\n",
       "      <td>40.671873</td>\n",
       "      <td>-73.954636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-01-20</td>\n",
       "      <td>19:20</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1010719.0</td>\n",
       "      <td>186857.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>transit</td>\n",
       "      <td>...</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>180.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>heavy</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2133381</td>\n",
       "      <td>40.679516</td>\n",
       "      <td>-73.904570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>19:50</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1014014.0</td>\n",
       "      <td>249325.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>175.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3425345</td>\n",
       "      <td>40.850964</td>\n",
       "      <td>-73.892414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-11-22</td>\n",
       "      <td>22:35</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1008499.0</td>\n",
       "      <td>191844.0</td>\n",
       "      <td>8631.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2650242</td>\n",
       "      <td>40.693211</td>\n",
       "      <td>-73.912556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        date   time  precinct     xcoord    ycoord  serial  radio_run  \\\n",
       "0  2009  2009-02-13  13:01       9.0   990071.0  204546.0   730.0      False   \n",
       "1  2010  2010-07-02  22:11      77.0   996834.0  184061.0  6369.0      False   \n",
       "3  2010  2010-01-20  19:20      73.0  1010719.0  186857.0   967.0      False   \n",
       "4  2012  2012-01-18  19:50      48.0  1014014.0  249325.0   334.0      False   \n",
       "5  2010  2010-11-22  22:35      83.0  1008499.0  191844.0  8631.0      False   \n",
       "\n",
       "   inside_outside location_housing  ...  suspect_height suspect_weight  \\\n",
       "0           False          neither  ...        6.166667          185.0   \n",
       "1           False          neither  ...        5.666667          175.0   \n",
       "3            True          transit  ...        5.666667          180.0   \n",
       "4           False          neither  ...        6.000000          175.0   \n",
       "5           False          neither  ...        5.750000          170.0   \n",
       "\n",
       "   suspect_hair suspect_eye  suspect_build  found_gun  found_weapon       id  \\\n",
       "0         black       brown         medium      False         False  1603997   \n",
       "1         black       brown         medium      False         False  2422905   \n",
       "3         black       brown          heavy      False         False  2133381   \n",
       "4         black       brown         medium      False         False  3425345   \n",
       "5         black       brown         medium      False         False  2650242   \n",
       "\n",
       "         lat        lon  \n",
       "0  40.728106 -73.978998  \n",
       "1  40.671873 -73.954636  \n",
       "3  40.679516 -73.904570  \n",
       "4  40.850964 -73.892414  \n",
       "5  40.693211 -73.912556  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ranging-edition"
   },
   "source": [
    "## Using regression to analyze the frisk decision. (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "canadian-signature"
   },
   "source": [
    "We will start by testing for racial discrimination in the decision to conduct a frisk after a stop: ie, whether Black and Hispanic pedestrians are more likely to be frisked (patted down for weapons) after they are stopped, controlling for other factors.\n",
    "\n",
    "a. Using statsmodels, perform a logistic regression, using `frisked` as the dependent variable and `suspect_race` as the independent variable, to assess how the probability of being frisked after a stop varies by race. Write a few sentences interpreting the results, making sure to answer the following questions: which value of suspect_race is omitted from the regression coefficients? Which race groups are most likely to be frisked after being stopped? How do you interpret the magnitude and sign of the coefficients? How do you interpret their statistical significance and confidence intervals? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         False\n",
      "1         False\n",
      "3         False\n",
      "4         False\n",
      "5         False\n",
      "          ...  \n",
      "408608    False\n",
      "408609    False\n",
      "408610    False\n",
      "408611    False\n",
      "408614    False\n",
      "Name: frisked_bc_weapons, Length: 273235, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#preview the data\n",
    "print (data['frisked_bc_weapons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        False  True \n",
      "0           1      0\n",
      "1           1      0\n",
      "3           1      0\n",
      "4           1      0\n",
      "5           1      0\n",
      "...       ...    ...\n",
      "408608      1      0\n",
      "408609      1      0\n",
      "408610      1      0\n",
      "408611      1      0\n",
      "408614      1      0\n",
      "\n",
      "[273235 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# converting data to dummies\n",
    "dummies = pd.get_dummies(data['frisked_bc_weapons'])\n",
    "print (dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column to data\n",
    "data['frisked_bc_weapons_dummies'] = dummies[1]\n",
    "# assigning a column for frisked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# previewing the data\n",
    "print (data['frisked_bc_weapons_dummies'].unique())\n",
    "# 0 = False\n",
    "# 1 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.150585\n",
      "         Iterations 7\n",
      "                               Logit Regression Results                               \n",
      "======================================================================================\n",
      "Dep. Variable:     frisked_bc_weapons_dummies   No. Observations:               273235\n",
      "Model:                                  Logit   Df Residuals:                   273232\n",
      "Method:                                   MLE   Df Model:                            2\n",
      "Date:                        Mon, 08 Nov 2021   Pseudo R-squ.:               0.0005370\n",
      "Time:                                09:58:03   Log-Likelihood:                -41145.\n",
      "converged:                               True   LL-Null:                       -41167.\n",
      "Covariance Type:                    nonrobust   LLR p-value:                 2.509e-10\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      -3.2722      0.014   -240.163      0.000      -3.299      -3.246\n",
      "C(suspect_race)[T.hispanic]    -0.1014      0.023     -4.430      0.000      -0.146      -0.057\n",
      "C(suspect_race)[T.white]       -0.2155      0.038     -5.695      0.000      -0.290      -0.141\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race)'\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Which value of suspect race is omitted from the regression coefficients? \n",
    "Which race groups are most likely to be frisked after being stopped? \n",
    "How do you interpret the magnitude and sign of the coefficients? How do you interpret their statistical significance and confidence intervals?\n",
    "\n",
    "From the logistic regression summary above, we can see that black is omitted from the regression coefficients, but that is only because it is noted as intercept and the other two races (i.e., Hispanic, and white) are evaluated with respect to the intercept. That being said, blacks are more likely than Hispanics and whites to be stopped and frisked for weapons. Looking at the coefficients, we see this. All the coefficients are negative meaning each race category is already negatively correlated with being stopped and frisked for weapons, but among these groups there is disparate impact. Looking at the coefficients, we see that blacks are more likely to be stopped and frisked, then Hispanics, and lastly whites. To quantify this further, by taking e^ (\"each race coefficient\"), we can better understand the impacts of race on being stopped and frisked for weapons. People who are stopped and frisked for weapons there is a .038 chance they are black, .034 they are Hispanic, and .031 they are white. The p-values for each of the race groups is less than .05 and are significant. The confidence intervals are narrow and from the large number of observations in that the results were based on, this is expected, and we can trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "earned-mainstream"
   },
   "source": [
    "b. Now perform a linear regression instead of a logistic regression using the same formula. How is the interpretation of the coefficients similar or different in the two regressions? What are the advantages of each? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1lTNBfBqgXMW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                OLS Regression Results                                \n",
      "======================================================================================\n",
      "Dep. Variable:     frisked_bc_weapons_dummies   R-squared:                       0.000\n",
      "Model:                                    OLS   Adj. R-squared:                  0.000\n",
      "Method:                         Least Squares   F-statistic:                     21.66\n",
      "Date:                        Mon, 08 Nov 2021   Prob (F-statistic):           3.91e-10\n",
      "Time:                                10:40:39   Log-Likelihood:                 76375.\n",
      "No. Observations:                      273235   AIC:                        -1.527e+05\n",
      "Df Residuals:                          273232   BIC:                        -1.527e+05\n",
      "Df Model:                                   2                                         \n",
      "Covariance Type:                    nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                       0.0365      0.000     78.116      0.000       0.036       0.037\n",
      "C(suspect_race)[T.hispanic]    -0.0034      0.001     -4.467      0.000      -0.005      -0.002\n",
      "C(suspect_race)[T.white]       -0.0069      0.001     -5.768      0.000      -0.009      -0.005\n",
      "==============================================================================\n",
      "Omnibus:                   265192.374   Durbin-Watson:                   2.004\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          7658601.268\n",
      "Skew:                           5.085   Prob(JB):                         0.00\n",
      "Kurtosis:                      26.860   Cond. No.                         3.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race)'\n",
    "lin_regress = smf.ols(formula=str(f), data=data).fit()\n",
    "print (lin_regress.summary())\n",
    "\n",
    "#lin_reg=sm.OLS(ytrain,xtrain).fit()\n",
    "#print(lin_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgk3tioqWpJg"
   },
   "source": [
    "### Answer\n",
    "\n",
    "How is the interpretation of the coefficients similar or different in the two regressions? What are the advantages of each?\n",
    "\n",
    "Both logistic and linear regression both show that blacks have the highest odds of being stopped and frisked for weapons compared to other race groups. Given the nature of the regressions, logistic regression provides a better interpretation of the data since we are trying to evaluate whether race groups have an impact on a binary outcome (stopped and frisked for weapons or NOT stopped and frisked for weapons). There is an inherent separation in the data that is best modeled by logistic regression and not linear regression. When groups are being compared to each other in a success / fail outcome, then logistic regression is best; when observing the collective correlation of features on the likelihood of an outcome, linear regression should be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "constitutional-place"
   },
   "source": [
    "c. The regression using only race as an independent variable is a good starting point, but it does not control for any other variables. What other variables do you think are important to control for, and why?  (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'date', 'time', 'precinct', 'xcoord', 'ycoord', 'serial', 'radio_run', 'inside_outside', 'location_housing', 'observation_period', 'suspected_crime', 'stop_length', 'identification', 'reason_explained', 'others_stopped', 'arrested', 'arrested_reason', 'summons_issued', 'officer_uniform', 'officer_verbal', 'officer_shield', 'frisked', 'searched', 'extra_reports', 'force_hands', 'force_wall', 'force_ground', 'force_drawn', 'force_pointed', 'force_baton', 'force_handcuffs', 'force_pepper', 'force_other', 'stopped_bc_object', 'stopped_bc_desc', 'stopped_bc_casing', 'stopped_bc_lookout', 'stopped_bc_clothing', 'stopped_bc_drugs', 'stopped_bc_furtive', 'stopped_bc_violent', 'stopped_bc_bulge', 'stopped_bc_other', 'frisked_bc_suspected_crime', 'frisked_bc_weapons', 'frisked_bc_attire', 'frisked_bc_actual_crime', 'frisked_bc_noncompliance', 'frisked_bc_threats', 'frisked_bc_prior', 'frisked_bc_furtive', 'frisked_bc_bulge', 'additional_report', 'additional_investigation', 'additional_proximity', 'additional_evasive', 'additional_associating', 'additional_direction', 'additional_highcrime', 'additional_time', 'additional_sights', 'additional_other', 'searched_hardobject', 'searched_outline', 'searched_admission', 'searched_other', 'found_contraband', 'found_pistol', 'found_rifle', 'found_assault', 'found_knife', 'found_machinegun', 'found_other', 'suspect_sex', 'suspect_race', 'suspect_hispanic', 'suspect_age', 'suspect_dob', 'suspect_height', 'suspect_weight', 'suspect_hair', 'suspect_eye', 'suspect_build', 'found_gun', 'found_weapon', 'id', 'lat', 'lon', 'frisked_bc_weapons_dummies']\n"
     ]
    }
   ],
   "source": [
    " # previewing the columns that could be incorporated\n",
    "print (list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "# exploring unique values in columns\n",
    "print (data['radio_run'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.150515\n",
      "         Iterations 7\n",
      "                               Logit Regression Results                               \n",
      "======================================================================================\n",
      "Dep. Variable:     frisked_bc_weapons_dummies   No. Observations:               273235\n",
      "Model:                                  Logit   Df Residuals:                   273231\n",
      "Method:                                   MLE   Df Model:                            3\n",
      "Date:                        Mon, 08 Nov 2021   Pseudo R-squ.:                0.001002\n",
      "Time:                                12:43:39   Log-Likelihood:                -41126.\n",
      "converged:                               True   LL-Null:                       -41167.\n",
      "Covariance Type:                    nonrobust   LLR p-value:                 8.914e-18\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      -3.3045      0.015   -225.251      0.000      -3.333      -3.276\n",
      "C(suspect_race)[T.hispanic]    -0.1108      0.023     -4.830      0.000      -0.156      -0.066\n",
      "C(suspect_race)[T.white]       -0.2366      0.038     -6.226      0.000      -0.311      -0.162\n",
      "C(radio_run)[T.True]            0.1498      0.024      6.257      0.000       0.103       0.197\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testing the variable\n",
    "\n",
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race) + C(radio_run)'\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS3NmINSWsDL"
   },
   "source": [
    "### Answer\n",
    "\n",
    "When considering disparate impacts in stop and frisk for weapons it is important to consider race. There are other aspects of that could be used to help explain who is disproportionately stopped and frisked. This includes radio_run. If someone is already run on the radio, they are actively looking for someone of a certain description which could mean that if someone is stopped and frisked, there is a reason for that. By incorporating this, we can compare remove those cases where someone was stopped because they fit a description that was already broadcasted and not for discriminatory reasons (i.e., can the disproportionate stopping of blacks and Hispanics be explained through radio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assumed-employment"
   },
   "source": [
    "d. Run a logistic regression where you control for both race and for the \"precinct\" variable, which encodes the police precinct in which the stop occurred. Make sure to control for precinct as a categorical, not a numerical, variable, by writing it as C(precinct) in the regression formula - why is this important to do?\n",
    "\n",
    "How do the race coefficients change, and what does that mean? How does the interpretation of this regression differ from the regression in which you only control for race? Make one argument in favor of reporting results controlling for location, and one argument against it. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.148051\n",
      "         Iterations 9\n",
      "                               Logit Regression Results                               \n",
      "======================================================================================\n",
      "Dep. Variable:     frisked_bc_weapons_dummies   No. Observations:               273235\n",
      "Model:                                  Logit   Df Residuals:                   273157\n",
      "Method:                                   MLE   Df Model:                           77\n",
      "Date:                        Mon, 08 Nov 2021   Pseudo R-squ.:                 0.01736\n",
      "Time:                                11:26:19   Log-Likelihood:                -40453.\n",
      "converged:                               True   LL-Null:                       -41167.\n",
      "Covariance Type:                    nonrobust   LLR p-value:                6.634e-248\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      -3.4831      0.161    -21.658      0.000      -3.798      -3.168\n",
      "C(suspect_race)[T.hispanic]    -0.0594      0.026     -2.286      0.022      -0.110      -0.008\n",
      "C(suspect_race)[T.white]        0.0560      0.042      1.319      0.187      -0.027       0.139\n",
      "C(precinct)[T.5.0]              0.5918      0.206      2.870      0.004       0.188       0.996\n",
      "C(precinct)[T.6.0]              0.1364      0.214      0.638      0.523      -0.283       0.555\n",
      "C(precinct)[T.7.0]              0.3535      0.201      1.756      0.079      -0.041       0.748\n",
      "C(precinct)[T.9.0]              0.6081      0.185      3.292      0.001       0.246       0.970\n",
      "C(precinct)[T.10.0]             0.5696      0.200      2.847      0.004       0.177       0.962\n",
      "C(precinct)[T.13.0]             0.7839      0.184      4.263      0.000       0.423       1.144\n",
      "C(precinct)[T.14.0]             0.4070      0.175      2.324      0.020       0.064       0.750\n",
      "C(precinct)[T.17.0]             0.5410      0.230      2.355      0.019       0.091       0.991\n",
      "C(precinct)[T.18.0]             0.3458      0.206      1.675      0.094      -0.059       0.750\n",
      "C(precinct)[T.19.0]             0.5536      0.189      2.924      0.003       0.182       0.925\n",
      "C(precinct)[T.20.0]            -0.0672      0.216     -0.311      0.756      -0.491       0.357\n",
      "C(precinct)[T.22.0]             0.1638      0.316      0.518      0.604      -0.455       0.783\n",
      "C(precinct)[T.23.0]             0.5688      0.169      3.357      0.001       0.237       0.901\n",
      "C(precinct)[T.24.0]             0.5920      0.190      3.110      0.002       0.219       0.965\n",
      "C(precinct)[T.25.0]             0.9940      0.171      5.812      0.000       0.659       1.329\n",
      "C(precinct)[T.26.0]             0.4856      0.190      2.556      0.011       0.113       0.858\n",
      "C(precinct)[T.28.0]             0.3737      0.181      2.068      0.039       0.020       0.728\n",
      "C(precinct)[T.30.0]             0.1456      0.193      0.755      0.450      -0.232       0.524\n",
      "C(precinct)[T.32.0]             0.5860      0.171      3.419      0.001       0.250       0.922\n",
      "C(precinct)[T.33.0]             0.1212      0.197      0.616      0.538      -0.264       0.507\n",
      "C(precinct)[T.34.0]             0.0483      0.187      0.258      0.796      -0.318       0.414\n",
      "C(precinct)[T.40.0]             0.3137      0.171      1.833      0.067      -0.022       0.649\n",
      "C(precinct)[T.41.0]             0.1386      0.184      0.754      0.451      -0.222       0.499\n",
      "C(precinct)[T.42.0]             0.6553      0.173      3.792      0.000       0.317       0.994\n",
      "C(precinct)[T.43.0]             0.8441      0.169      4.981      0.000       0.512       1.176\n",
      "C(precinct)[T.44.0]             0.4357      0.172      2.532      0.011       0.098       0.773\n",
      "C(precinct)[T.45.0]             0.2005      0.201      1.000      0.317      -0.192       0.594\n",
      "C(precinct)[T.46.0]            -0.1408      0.186     -0.757      0.449      -0.506       0.224\n",
      "C(precinct)[T.47.0]             0.6131      0.175      3.501      0.000       0.270       0.956\n",
      "C(precinct)[T.48.0]             0.1843      0.203      0.909      0.363      -0.213       0.581\n",
      "C(precinct)[T.49.0]            -0.3086      0.201     -1.535      0.125      -0.703       0.085\n",
      "C(precinct)[T.50.0]             0.2464      0.226      1.089      0.276      -0.197       0.690\n",
      "C(precinct)[T.52.0]             0.7235      0.173      4.184      0.000       0.385       1.062\n",
      "C(precinct)[T.60.0]            -0.0551      0.186     -0.296      0.767      -0.420       0.310\n",
      "C(precinct)[T.61.0]            -0.4223      0.209     -2.023      0.043      -0.832      -0.013\n",
      "C(precinct)[T.62.0]            -0.7181      0.237     -3.034      0.002      -1.182      -0.254\n",
      "C(precinct)[T.63.0]             0.3537      0.205      1.724      0.085      -0.048       0.756\n",
      "C(precinct)[T.66.0]             0.6278      0.197      3.187      0.001       0.242       1.014\n",
      "C(precinct)[T.67.0]             0.3497      0.175      1.999      0.046       0.007       0.693\n",
      "C(precinct)[T.68.0]             0.2006      0.219      0.917      0.359      -0.228       0.629\n",
      "C(precinct)[T.69.0]            -0.1579      0.199     -0.794      0.427      -0.548       0.232\n",
      "C(precinct)[T.70.0]             0.0153      0.180      0.085      0.932      -0.338       0.368\n",
      "C(precinct)[T.71.0]            -0.7155      0.221     -3.235      0.001      -1.149      -0.282\n",
      "C(precinct)[T.72.0]            -0.0929      0.208     -0.446      0.656      -0.501       0.315\n",
      "C(precinct)[T.73.0]             0.0221      0.170      0.130      0.897      -0.311       0.355\n",
      "C(precinct)[T.75.0]            -0.0506      0.169     -0.299      0.765      -0.382       0.280\n",
      "C(precinct)[T.76.0]            -0.4767      0.221     -2.155      0.031      -0.910      -0.043\n",
      "C(precinct)[T.77.0]             0.1140      0.179      0.635      0.525      -0.238       0.466\n",
      "C(precinct)[T.78.0]            -0.1456      0.239     -0.610      0.542      -0.613       0.322\n",
      "C(precinct)[T.79.0]             0.0665      0.173      0.385      0.701      -0.272       0.405\n",
      "C(precinct)[T.81.0]            -0.2479      0.185     -1.337      0.181      -0.611       0.115\n",
      "C(precinct)[T.83.0]            -0.3084      0.187     -1.647      0.100      -0.675       0.059\n",
      "C(precinct)[T.84.0]             0.4511      0.194      2.324      0.020       0.071       0.832\n",
      "C(precinct)[T.88.0]             0.2293      0.186      1.233      0.218      -0.135       0.594\n",
      "C(precinct)[T.90.0]            -0.1448      0.181     -0.799      0.424      -0.500       0.210\n",
      "C(precinct)[T.94.0]             0.0698      0.246      0.284      0.776      -0.412       0.551\n",
      "C(precinct)[T.100.0]            0.0609      0.208      0.293      0.770      -0.347       0.469\n",
      "C(precinct)[T.101.0]            0.3246      0.177      1.834      0.067      -0.022       0.671\n",
      "C(precinct)[T.102.0]            0.1473      0.191      0.770      0.441      -0.227       0.522\n",
      "C(precinct)[T.103.0]            0.3799      0.172      2.205      0.027       0.042       0.717\n",
      "C(precinct)[T.104.0]           -0.8514      0.225     -3.783      0.000      -1.293      -0.410\n",
      "C(precinct)[T.105.0]           -0.0784      0.189     -0.414      0.679      -0.449       0.292\n",
      "C(precinct)[T.106.0]           -0.1274      0.208     -0.613      0.540      -0.535       0.280\n",
      "C(precinct)[T.107.0]           -0.3712      0.216     -1.719      0.086      -0.794       0.052\n",
      "C(precinct)[T.108.0]           -0.5924      0.225     -2.631      0.009      -1.034      -0.151\n",
      "C(precinct)[T.109.0]           -0.6485      0.206     -3.154      0.002      -1.051      -0.246\n",
      "C(precinct)[T.110.0]           -0.2025      0.186     -1.087      0.277      -0.568       0.163\n",
      "C(precinct)[T.111.0]           -0.3936      0.235     -1.676      0.094      -0.854       0.067\n",
      "C(precinct)[T.112.0]           -0.5352      0.262     -2.046      0.041      -1.048      -0.023\n",
      "C(precinct)[T.113.0]            0.0737      0.181      0.407      0.684      -0.281       0.428\n",
      "C(precinct)[T.114.0]           -0.2201      0.187     -1.176      0.240      -0.587       0.147\n",
      "C(precinct)[T.115.0]            0.1273      0.177      0.718      0.472      -0.220       0.475\n",
      "C(precinct)[T.120.0]           -0.2008      0.176     -1.143      0.253      -0.545       0.143\n",
      "C(precinct)[T.122.0]           -1.3217      0.242     -5.452      0.000      -1.797      -0.847\n",
      "C(precinct)[T.123.0]           -0.7442      0.288     -2.583      0.010      -1.309      -0.180\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race) + C(precinct)'\n",
    "\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xddOFV6kWvzy"
   },
   "source": [
    "### Answer\n",
    "\n",
    "It is important to code for precinct as a categorical variable over a numerical variables because each precinct is not related linearly. Putting them into the logistic regression without the \"C\", will not render the correct output. The race coefficients change; the coefficients decrease for blacks and Hispanics, but increases for whites, when accounting for precinct. This proposes a different story.\n",
    "\n",
    "#### For Reporting Location Results\n",
    "When reporting this data accounting for precinct we see that whites are stopped and frisked for weapons more than other race groups. This explains some of the high rates of stopping and frisking people of color, nonetheless this brings about a new observation - that some precincts over others are more likely to stop and frisk. These precincts may also be places that are overly surveilled. This indicates that there is a need to focus on the disparate impacts across police precincts and investigate why this is so.\n",
    "\n",
    "\n",
    "#### Not For Reporting Location Results\n",
    "When reporting this data without accounting for precinct, we can see that blacks and Hispanics have higher odds of being stopped and frisked by police for weapons. This can show that there might be an issue with implicit bias among police officers.\n",
    "\n",
    "#### Bringing These Together\n",
    "I would choose to report both. Overall, we can see that blacks and Hispanics are stopped at higher rates than white people, but when accounting for precinct, this is no longer the story. However, there are precincts that have higher rates for police stop and frisk for weapons. A question that arises from this includes are stop and frisk for weapons motivated by implicit biases against Blacks and Hispanics, which then pushed for increased stop and frisk for weapons rates in majority Black and Hispanic police precincts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.150585\n",
      "         Iterations 7\n",
      "                               Logit Regression Results                               \n",
      "======================================================================================\n",
      "Dep. Variable:     frisked_bc_weapons_dummies   No. Observations:               273235\n",
      "Model:                                  Logit   Df Residuals:                   273232\n",
      "Method:                                   MLE   Df Model:                            2\n",
      "Date:                        Mon, 08 Nov 2021   Pseudo R-squ.:               0.0005370\n",
      "Time:                                11:25:12   Log-Likelihood:                -41145.\n",
      "converged:                               True   LL-Null:                       -41167.\n",
      "Covariance Type:                    nonrobust   LLR p-value:                 2.509e-10\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      -3.2722      0.014   -240.163      0.000      -3.299      -3.246\n",
      "C(suspect_race)[T.hispanic]    -0.1014      0.023     -4.430      0.000      -0.146      -0.057\n",
      "C(suspect_race)[T.white]       -0.2155      0.038     -5.695      0.000      -0.290      -0.141\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race)'\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "expected-whole"
   },
   "source": [
    "e. In a few sentences, explain why it is a BAD idea, conceptually, to control for the following variables if we are trying to assess whether the police racially discriminate in whom they frisk after a stop: a) \"found.weapon\", which encodes whether the frisk found a weapon, and b) \"suspect.eye\" and \"suspect.hair\", which encode the suspect's eye and hair color. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5dWS0LAW6De"
   },
   "source": [
    "### Answer\n",
    "\n",
    "a.) It is a bad idea to control for \"found weapon\" because in doing so, it does not help explain whether different racial groups are disparately stopped and frisked for weapons, but instead, validates discriminatory stop and frisks if the suspect had a weapon.\n",
    "\n",
    "b.) It would also be a bad idea to account for features such as eye color and hair because these visual features could account for some variation in the model, but these features are often associated with race. E.g., white people are more likely to have green/blue/etc. colored eyes, and blonde hair, while Blacks and Hispanics are likely to have darker colored hair and brown / dark brown eyes. The results here might be very similar to what we had before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "closed-difference"
   },
   "source": [
    "f. In a few sentences, explain the problem of omitted variable bias in this analysis, and how it would undermine the conclusions. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74cAqUPgW7Mu"
   },
   "source": [
    "### Answer\n",
    "Depending on the story that someone wants to prove through the data, there can be a bias to include or not include certain variables from the analysis. As seen with the inclusion of \"precinct\" the results changed to show that actually whites are more likely to be stopped. Although this is true in that analysis, when we don't account for precinct, we see that blacks are at a higher risk than Hispanics and whites for being stopped and frisked for weapons.\n",
    "\n",
    "In another case, someone who does not have domain expertise my unknowingly include or omit variables that could misrepresent what is going on. Suppose if someone does not include race in this analysis, they could replace that with hair color and show that people with darker hair colors are stopped more frequently than those who do not. The conclusion here would report that it is hair color that explains the basis for discrimination for police.\n",
    "\n",
    "With a topics as controversial as racial discrimination, no one wants to be labeled as racially discriminatory, so they will try to shape an analysis to disprove that with the methods in data analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBwIUjuRt5o2"
   },
   "source": [
    "g. In a few sentences, explain why only examining whether someone is frisked after a stop might fail to provide a full picture of discrimination in the stop and frisk policy. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuBEAzCVW7zQ"
   },
   "source": [
    "### Answer\n",
    "Examining whether someone is frisked after a stop fails to provide a full picture of discrimination because the reason for initial the stop could be motivated by a myriad of reasons, which then called for a frisk. This is different from stop and frisk in that the motivation for it was mere suspicion and not a defined violation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grateful-chassis"
   },
   "source": [
    "## Outcome analysis using regularized regression (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scientific-franchise"
   },
   "source": [
    "Because of the issues with omitted variables in analyses like the one above, *outcome* tests are often used: these look not at the rate at which a decision is made (like the decision to frisk), but at the outcome of the decision (for example, if the frisk is conducted to find a weapon, does it actually find one?) Now we will use an outcome-style analysis. Specifically, we will fit a machine learning model to predict the probability that each stop which was conducted on suspicion the pedestrian possessed a weapon actually finds a weapon. Stops which are very unlikely to find a weapon arguably violate the Fourth Amendment, which prohibits unreasonable searches; if such stops disproportionately occur of certain race groups, the policy may violate the Fourteenth Amendment, which prohibits racial discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CaMrTP0NlPY"
   },
   "source": [
    "a. In this portion of this analysis, you will be using a smaller version of the data to speed up model fitting. Load in \"small_sqf_sample\". As before, we need to do some data processing for consistency with previous of analysis of this data. Load in the data and filter for stops between 2008 and 2012 (including both 2008 and 2012); filter for stops of white, Black, and Hispanic pedestrians using the suspect_race column; and filter for stops conducted on suspicion of criminal posession of a weapon (ie, suspected_crime == 'cpw'). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cDZV0_2ygaOM"
   },
   "outputs": [],
   "source": [
    "# loading the data in\n",
    "sample_data = pd.read_csv(\"small_sqf_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cDZV0_2ygaOM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2009 2010 2012 2008 2011]\n",
      "['black' 'white' 'hispanic']\n",
      "['cpw']\n"
     ]
    }
   ],
   "source": [
    "# filtering for year\n",
    "sample_data = sample_data[sample_data[\"year\"]>=2008]\n",
    "sample_data = sample_data[sample_data[\"year\"]<=2012]\n",
    "print (data[\"year\"].unique())\n",
    "\n",
    "# filtering for race\n",
    "selected = ['black', 'hispanic', 'white']\n",
    "sample_data = sample_data[sample_data[\"suspect_race\"].isin(selected)]\n",
    "# seeing the unique values in suspect_race\n",
    "print (sample_data[\"suspect_race\"].unique())\n",
    "\n",
    "# filtering for suspected crime\n",
    "sample_data = sample_data[sample_data[\"suspected_crime\"]==\"cpw\"]\n",
    "print (sample_data[\"suspected_crime\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "confident-array"
   },
   "source": [
    "\n",
    "b. We will be fitting the regression model \n",
    "\n",
    "`found_weapon ~ C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex + ADDITIONAL_CIRCUMSTANCE_COLUMNS` \n",
    "\n",
    "where ADDITIONAL_CIRCUMSTANCE_COLUMNS are any columns that begin with \"stopped_bc\" or \"additional_\" besides \"additional_other\" and \"stopped_bc_other\". You can get these columns by running\n",
    "\n",
    "`ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in d.columns if ('stopped_bc' in a or 'additional_' in a) and a not in (['additional_other', 'stopped_bc_other'])]`\n",
    "\n",
    "You should have 18 additional columns. These columns provide more information about the circumstances of the stop, and we include them for consistency with the original analysis and because they turn out to be important for predictive performance. \n",
    "\n",
    "Drop any rows with missing values in any of the variables you need. \n",
    "\n",
    "Now, we need to put the data into a format which sklearn can use later - ie, numpy arrays. Do this with \"patsy\" library and the \"dmatrix\" function. You can call dmatrix as follows:\n",
    "\n",
    "`sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS),sqf_data, return_type='dataframe')`\n",
    "\n",
    "and it will return a dataframe on which you can fit a regression model. The first argument to dmatrix is the formula that you want to use to make the dataframe; the second argument gives patsy the data you want to use; return_type='dataframe' ensures that you get a dataframe, not a patsy object which is hard to use.\n",
    "\n",
    "Look at the output of dmatrix and explain what the columns mean. Why can't we just pass the columns from the original dataframe directly into the sklearn function? (8 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the additional columns\n",
    "ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in sample_data.columns if ('stopped_bc' in a or 'additional_' in a) and a not in (['additional_other', 'stopped_bc_other'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "n42o63nLgdET",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['stopped_bc_object', 'stopped_bc_desc', 'stopped_bc_casing', 'stopped_bc_lookout', 'stopped_bc_clothing', 'stopped_bc_drugs', 'stopped_bc_furtive', 'stopped_bc_violent', 'stopped_bc_bulge', 'additional_report', 'additional_investigation', 'additional_proximity', 'additional_evasive', 'additional_associating', 'additional_direction', 'additional_highcrime', 'additional_time', 'additional_sights']\n"
     ]
    }
   ],
   "source": [
    "#check the len of the other ADDITIONAL_CIRCUMSTANCE_COLUMNS\n",
    "print (len(ADDITIONAL_CIRCUMSTANCE_COLUMNS))\n",
    "print (ADDITIONAL_CIRCUMSTANCE_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36056, 27)\n"
     ]
    }
   ],
   "source": [
    "#creating a new data frame with desired columns\n",
    "desired_columns=sample_data[[\"found_weapon\", \"precinct\", \"suspect_race\", \"location_housing\", \"year\", \"suspect_age\",\n",
    "                             \"suspect_height\", \"suspect_weight\", \"suspect_sex\", 'stopped_bc_object', 'stopped_bc_desc', \n",
    "                             'stopped_bc_casing', 'stopped_bc_lookout', 'stopped_bc_clothing', 'stopped_bc_drugs', \n",
    "                             'stopped_bc_furtive', 'stopped_bc_violent', 'stopped_bc_bulge', 'additional_report', 'additional_investigation', \n",
    "                             'additional_proximity', 'additional_evasive', 'additional_associating', 'additional_direction',\n",
    "                             'additional_highcrime', 'additional_time', 'additional_sights']]\n",
    "\n",
    "desired_data = desired_columns.dropna()\n",
    "print (desired_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS),sqf_data, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3N-RXXKX1PQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAib9e-WfUdF"
   },
   "source": [
    "c. As discussed in class, when fitting machine learning models, you should always divide the dataset into a train, val, and test set. Randomly divide the filtered, processed data into three pieces - the train set (60%), the val set (20%) and the test set (20%). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ditQALf_geMN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWZp2Jw7TKVm"
   },
   "source": [
    "d. We will be training a regularized logistic regression model to predict the outcome. When using many machine learning models, including regularized logistic regression, it is important to preprocess the input features so they are all on the same scale, for reasons discussed in class. In this case, we will take each column in the input data, subtract its mean, and divide by its standard deviation. This makes it so each column of the data has mean 0 and standard deviation 1. \n",
    "\n",
    "When scaling the data, it is important to compute the scaling using only the train set, as shown in class. The reason is that we are pretending that the train data is all we have access to to fit our model fitting pipeline, so we cannot \"peek\" at the validation or test sets to generate our scaling. Use sklearn's StandardScaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to fit a scaling transform on the train set and transform the train set (you can use the fit_transform method on the train set).Then apply the fitted StandardScaler model to the validation and test sets as well (using the transform --- not the fit --- method). (Look at the notebook we went through in class on regularization and lasso if you are confused.) The transformed datasets are the final datasets you will feed into your logistic regression model. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyVKwlbjgffV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "involved-implement"
   },
   "source": [
    "e. Uisng sklearn.linear_model.LogisticRegression, fit a model on the train set to predict found_weapon. Set the \"penalty\" argument to \"none\" so that the model will not use any regularization; this corresponds to fitting a regular logistic regression model. If you get a `ConvergenceWarning`, increase the number of iterations using the `max_iter` argument; this means the model optimization needs more iterations to converge. (Look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) if you are uncertain which arguments to use!)\n",
    "\n",
    "A standard measure of predictive performance for binary outcome variables like found_weapon is AUC. Higher values of AUC are better; an AUC of 1 means that the model is perfectly predicting the outcome; an AUC of 0.5 means that it is predicting it only as well as random chance. Report the AUC of the fitted model (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) on both the train and validation sets. What is the problem with using accuracy as a metric for this task? How does the train set AUC differ from the validation AUC, and does this make sense? (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExHdN87ggimI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ep5iHbBhn1z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eight-absorption"
   },
   "source": [
    "f. Our logistic regression model is not using any regularization and appears to be overfitting. We will try to use regularization to reduce overfitting. You will be fitting an L1-penalized logistic regression model using code that looks something like\n",
    "\n",
    "`LogisticRegression(C=sparsity_param, penalty='l1', solver='liblinear')`\n",
    "\n",
    "(the \"l1\" specifies that we're using an L1 penalty, as discussed in class; the \"liblinear\" solver is an optimizer that works with the L1 penalty. See the logistic regression documentation for more details.) \n",
    "\n",
    "Increase and decrease the amount of regularization using different values of the C parameter, searching logarithmically over at least 20 values in the range from 1e-2 to 1. (Note: we do not recommend defining a variable named \"C\" in your code, because this may cause weird patsy issues, since patsy also uses \"C\". Give the variable another name. Sorry! I complained to the patsy people.) \n",
    "\n",
    "Print out the train set, val set, and test set AUC for each value of of the regularization parameter. Make a plot where the x-axis is the regularization parameter and the y-axis is AUC, with one line for train AUC, one line for val AUC, and one line for test AUC (use plt.semilogx to plot the lines so the x-axis will be logarithmic, making it easier to see the plot). Comment on the trends. Do you see evidence of overfitting? Explain. For the rest of this assignment, use the model with the highest AUC on the validation set. (10 points)\n",
    "\n",
    "In a full analysis, it would make sense to play with other aspects of the model as well: for example, you could try using other forms of regularization (like L1 vs L2) or other classification algorithms besides logistic regression. The basic pattern, though, would be the same: fit the model on the train set, choose models on the val set, and once you've chosen your best model, assess your results (once!) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owMXSAmTeInj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "antique-connection"
   },
   "source": [
    "g. Assess model *calibration* on the test set. This checks whether the model's predicted probabilities line up with the true probabilities, and is important to assess here because we will be analyzing the model's predicted probabilities. To assess calibration, take the 10% of rows of the test set with the highest model predictions and compare the mean model predicted probability (ie, the output of `predict_proba`) to the actual mean of the outcome variable. Repeat for the next 10% of rows, and for all 10% groups. Make a plot comparing the model predicted probabilities on each 10% group to the actual outcomes (mean predicted probability on the x-axis and mean actual outcome on the y-axis). You should end up with a plot with 10 points, one for each 10% group. Plot the line y=x so you can see how well the predicted probabilities line up with the actual probabilities - ideally, your points will lie on the line or close to it. (10 points)\n",
    "\n",
    "Calibration is an issue for many machine learning models, including deep learning models, so this is always a good thing to check. In a full analysis, it would make sense to check calibration (and AUC) for subgroups as well (eg, each race group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frozen-apollo"
   },
   "source": [
    "h. Using the test set, compute the fraction of observations which have lower than a 2% model-predicted probability of finding a weapon for Black, Hispanic, and white pedestrians. Stops below this threshold are extremely unlikely to have resulted in finding a weapon, arguably violating the Fourth Amendment. Repeat this for 20 thresholds evenly spaced between 1% and 5%. Make a graph where the x-axis is the threshold, and the y-axis is the fraction of stops falling below that threshold, with one line for each race group. Explain what you observe. Do you think this graph provides evidence for Fourteenth Amendment violations (racial discrimination)? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "organized-luxembourg"
   },
   "source": [
    "i. The analysis you have performed in the second part of this assignment is very similar to the analysis in [this paper](https://5harad.com/papers/stop-and-frisk.pdf). Read the paper and write a few sentences about their main conclusions. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wMU2Jzse3Dm"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
