{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119aba6c",
   "metadata": {
    "id": "bizarre-tonight"
   },
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e4147",
   "metadata": {
    "id": "bizarre-tonight"
   },
   "source": [
    "Isaiah D. Murray (idm22@cornell.edu)\n",
    "\n",
    "Daan van der Zwagg (dv239@cornell.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5784a",
   "metadata": {
    "id": "patent-china"
   },
   "source": [
    "#### In this assignment, we will conclude our analysis of whether the stop and frisk policy was racially discriminatory, but from a very different angle than our previous mapping analysis. We will rely heavily on logistic regression and regularized regression, as discussed in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c13cff",
   "metadata": {
    "id": "lPy-JHVGfPsF"
   },
   "source": [
    "# Data processing (5 points)\n",
    "\n",
    "First we need to do some data processing for consistency with previous of analysis of stop_and_frisk data. Load in the data \"sqf_sample\" and filter for stops between 2008 and 2012 (including both 2008 and 2012 in your sample). Filter for stops of white, Black, and Hispanic pedestrians using the suspect_race column. (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8017843d",
   "metadata": {
    "id": "Kd54AVi-gUOq"
   },
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3099ba65",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sqf_sample.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6308c217b554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqf_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sqf_sample.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sqf_sample.csv'"
     ]
    }
   ],
   "source": [
    "# read in csv\n",
    "sqf_sample = pd.read_csv(\"sqf_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605f988f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqf_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-40fae8bffcd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqf_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sqf_sample' is not defined"
     ]
    }
   ],
   "source": [
    "data = sqf_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56639ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-28dd3be9b55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# check shape\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0d902d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a4286c860726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99033db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4885f7743d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preview the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5996cbd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e79383b91f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# filtering for year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m2008\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2012\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# filtering for year\n",
    "data = data[data[\"year\"]>=2008]\n",
    "data = data[data[\"year\"]<=2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b30c0626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-58099d0432ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#checking the data shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#checking the data shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41848378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# making sure that there are in fact only years 2008 to 2012\n",
    "print (data[\"year\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de675f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seeing the unique values in suspect_race\n",
    "print (data[\"suspect_race\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for race\n",
    "selected = ['black', 'hispanic', 'white']\n",
    "data = data[data[\"suspect_race\"].isin(selected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c8807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seeing the unique values in suspect_race\n",
    "print (data[\"suspect_race\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fb14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385642b",
   "metadata": {
    "id": "ranging-edition"
   },
   "source": [
    "## Using regression to analyze the frisk decision. (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b70bcb",
   "metadata": {
    "id": "canadian-signature"
   },
   "source": [
    "We will start by testing for racial discrimination in the decision to conduct a frisk after a stop: ie, whether Black and Hispanic pedestrians are more likely to be frisked (patted down for weapons) after they are stopped, controlling for other factors.\n",
    "\n",
    "a. Using statsmodels, perform a logistic regression, using `frisked` as the dependent variable and `suspect_race` as the independent variable, to assess how the probability of being frisked after a stop varies by race. Write a few sentences interpreting the results, making sure to answer the following questions: which value of suspect_race is omitted from the regression coefficients? Which race groups are most likely to be frisked after being stopped? How do you interpret the magnitude and sign of the coefficients? How do you interpret their statistical significance and confidence intervals? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview the data\n",
    "print (data['frisked_bc_weapons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd4ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting data to dummies\n",
    "dummies = pd.get_dummies(data['frisked_bc_weapons'])\n",
    "print (dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column to data\n",
    "data['frisked_bc_weapons_dummies'] = dummies[1]\n",
    "# assigning a column for frisked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866623c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# previewing the data\n",
    "print (data['frisked_bc_weapons_dummies'].unique())\n",
    "# 0 = False\n",
    "# 1 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c58ef4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race)'\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a4637",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Which value of suspect race is omitted from the regression coefficients? \n",
    "Which race groups are most likely to be frisked after being stopped? \n",
    "How do you interpret the magnitude and sign of the coefficients? How do you interpret their statistical significance and confidence intervals?\n",
    "\n",
    "From the logistic regression summary above, we can see that black is omitted from the regression coefficients, but that is only because it is noted as intercept and the other two races (i.e., Hispanic, and white) are evaluated with respect to the intercept. That being said, blacks are more likely than Hispanics and whites to be stopped and frisked for weapons. Looking at the coefficients, we see this. All the coefficients are negative meaning each race category is already negatively correlated with being stopped and frisked for weapons, but among these groups there is disparate impact. Looking at the coefficients, we see that blacks are more likely to be stopped and frisked, then Hispanics, and lastly whites. To quantify this further, by taking e^ (\"each race coefficient\"), we can better understand the impacts of race on being stopped and frisked for weapons. People who are stopped and frisked for weapons there is a .038 chance they are black, .034 they are Hispanic, and .031 they are white. The p-values for each of the race groups is less than .05 and are significant. The confidence intervals are narrow and from the large number of observations in that the results were based on, this is expected, and we can trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0addbff",
   "metadata": {
    "id": "earned-mainstream"
   },
   "source": [
    "b. Now perform a linear regression instead of a logistic regression using the same formula. How is the interpretation of the coefficients similar or different in the two regressions? What are the advantages of each? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dd926",
   "metadata": {
    "id": "1lTNBfBqgXMW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#linear regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race)'\n",
    "lin_regress = smf.ols(formula=str(f), data=data).fit()\n",
    "print (lin_regress.summary())\n",
    "\n",
    "#lin_reg=sm.OLS(ytrain,xtrain).fit()\n",
    "#print(lin_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582b009",
   "metadata": {
    "id": "rgk3tioqWpJg"
   },
   "source": [
    "### Answer\n",
    "\n",
    "How is the interpretation of the coefficients similar or different in the two regressions? What are the advantages of each?\n",
    "\n",
    "Both logistic and linear regression both show that blacks have the highest odds of being stopped and frisked for weapons compared to other race groups. Given the nature of the regressions, logistic regression provides a better interpretation of the data since we are trying to evaluate whether race groups have an impact on a binary outcome (stopped and frisked for weapons or NOT stopped and frisked for weapons). There is an inherent separation in the data that is best modeled by logistic regression and not linear regression. When groups are being compared to each other in a success / fail outcome, then logistic regression is best; when observing the collective correlation of features on the likelihood of an outcome, linear regression should be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece9539",
   "metadata": {
    "id": "constitutional-place"
   },
   "source": [
    "c. The regression using only race as an independent variable is a good starting point, but it does not control for any other variables. What other variables do you think are important to control for, and why?  (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c592af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # previewing the columns that could be incorporated\n",
    "print (list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49fa74ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20fe3c3aa893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# exploring unique values in columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'radio_run'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# exploring unique values in columns\n",
    "print (data['radio_run'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4fb69f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f4c8834f6b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frisked_bc_weapons_dummies ~ C(suspect_race) + C(radio_run)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogitfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogitfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# testing the variable\n",
    "\n",
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race) + C(radio_run)'\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c471da",
   "metadata": {
    "id": "pS3NmINSWsDL"
   },
   "source": [
    "### Answer\n",
    "\n",
    "When considering disparate impacts in stop and frisk for weapons it is important to consider race. There are other aspects of that could be used to help explain who is disproportionately stopped and frisked. This includes radio_run. If someone is already run on the radio, they are actively looking for someone of a certain description which could mean that if someone is stopped and frisked, there is a reason for that. By incorporating this, we can compare remove those cases where someone was stopped because they fit a description that was already broadcasted and not for discriminatory reasons (i.e., can the disproportionate stopping of blacks and Hispanics be explained through radio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6df2f",
   "metadata": {
    "id": "assumed-employment"
   },
   "source": [
    "d. Run a logistic regression where you control for both race and for the \"precinct\" variable, which encodes the police precinct in which the stop occurred. Make sure to control for precinct as a categorical, not a numerical, variable, by writing it as C(precinct) in the regression formula - why is this important to do?\n",
    "\n",
    "How do the race coefficients change, and what does that mean? How does the interpretation of this regression differ from the regression in which you only control for race? Make one argument in favor of reporting results controlling for location, and one argument against it. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc984848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f47741705ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frisked_bc_weapons_dummies ~ C(suspect_race) + C(precinct)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogitfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogitfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race) + C(precinct)'\n",
    "\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3747b16",
   "metadata": {
    "id": "xddOFV6kWvzy"
   },
   "source": [
    "### Answer\n",
    "\n",
    "It is important to code for precinct as a categorical variable over a numerical variables because each precinct is not related linearly. Putting them into the logistic regression without the \"C\", will not render the correct output. The race coefficients change; the coefficients decrease for blacks and Hispanics, but increases for whites, when accounting for precinct. This proposes a different story.\n",
    "\n",
    "#### For Reporting Location Results\n",
    "When reporting this data accounting for precinct we see that whites are stopped and frisked for weapons more than other race groups. This explains some of the high rates of stopping and frisking people of color, nonetheless this brings about a new observation - that some precincts over others are more likely to stop and frisk. These precincts may also be places that are overly surveilled. This indicates that there is a need to focus on the disparate impacts across police precincts and investigate why this is so.\n",
    "\n",
    "\n",
    "#### Not For Reporting Location Results\n",
    "When reporting this data without accounting for precinct, we can see that blacks and Hispanics have higher odds of being stopped and frisked by police for weapons. This can show that there might be an issue with implicit bias among police officers.\n",
    "\n",
    "#### Bringing These Together\n",
    "I would choose to report both. Overall, we can see that blacks and Hispanics are stopped at higher rates than white people, but when accounting for precinct, this is no longer the story. However, there are precincts that have higher rates for police stop and frisk for weapons. A question that arises from this includes are stop and frisk for weapons motivated by implicit biases against Blacks and Hispanics, which then pushed for increased stop and frisk for weapons rates in majority Black and Hispanic police precincts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ad74f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1d8bac63ab6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frisked_bc_weapons_dummies ~ C(suspect_race)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogitfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogitfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "f = 'frisked_bc_weapons_dummies ~ C(suspect_race)'\n",
    "logitfit = smf.logit(formula = str(f), data = data).fit()\n",
    "print(logitfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564103ff",
   "metadata": {
    "id": "expected-whole"
   },
   "source": [
    "e. In a few sentences, explain why it is a BAD idea, conceptually, to control for the following variables if we are trying to assess whether the police racially discriminate in whom they frisk after a stop: a) \"found.weapon\", which encodes whether the frisk found a weapon, and b) \"suspect.eye\" and \"suspect.hair\", which encode the suspect's eye and hair color. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1899c0b",
   "metadata": {
    "id": "_5dWS0LAW6De"
   },
   "source": [
    "### Answer\n",
    "\n",
    "a.) It is a bad idea to control for \"found weapon\" because in doing so, it does not help explain whether different racial groups are disparately stopped and frisked for weapons, but instead, validates discriminatory stop and frisks if the suspect had a weapon.\n",
    "\n",
    "b.) It would also be a bad idea to account for features such as eye color and hair because these visual features could account for some variation in the model, but these features are often associated with race. E.g., white people are more likely to have green/blue/etc. colored eyes, and blonde hair, while Blacks and Hispanics are likely to have darker colored hair and brown / dark brown eyes. The results here might be very similar to what we had before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664fd43",
   "metadata": {
    "id": "closed-difference"
   },
   "source": [
    "f. In a few sentences, explain the problem of omitted variable bias in this analysis, and how it would undermine the conclusions. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14befdf9",
   "metadata": {
    "id": "74cAqUPgW7Mu"
   },
   "source": [
    "### Answer\n",
    "Depending on the story that someone wants to prove through the data, there can be a bias to include or not include certain variables from the analysis. As seen with the inclusion of \"precinct\" the results changed to show that actually whites are more likely to be stopped. Although this is true in that analysis, when we don't account for precinct, we see that blacks are at a higher risk than Hispanics and whites for being stopped and frisked for weapons.\n",
    "\n",
    "In another case, someone who does not have domain expertise my unknowingly include or omit variables that could misrepresent what is going on. Suppose if someone does not include race in this analysis, they could replace that with hair color and show that people with darker hair colors are stopped more frequently than those who do not. The conclusion here would report that it is hair color that explains the basis for discrimination for police.\n",
    "\n",
    "With a topics as controversial as racial discrimination, no one wants to be labeled as racially discriminatory, so they will try to shape an analysis to disprove that with the methods in data analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd9611",
   "metadata": {
    "id": "mBwIUjuRt5o2"
   },
   "source": [
    "g. In a few sentences, explain why only examining whether someone is frisked after a stop might fail to provide a full picture of discrimination in the stop and frisk policy. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332d5f1",
   "metadata": {
    "id": "IuBEAzCVW7zQ"
   },
   "source": [
    "### Answer\n",
    "Examining whether someone is frisked after a stop fails to provide a full picture of discrimination because the reason for initial the stop could be motivated by a myriad of reasons, which then called for a frisk. This is different from stop and frisk in that the motivation for it was mere suspicion and not a defined violation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343702b",
   "metadata": {
    "id": "grateful-chassis"
   },
   "source": [
    "## Outcome analysis using regularized regression (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4f8d0",
   "metadata": {
    "id": "scientific-franchise"
   },
   "source": [
    "Because of the issues with omitted variables in analyses like the one above, *outcome* tests are often used: these look not at the rate at which a decision is made (like the decision to frisk), but at the outcome of the decision (for example, if the frisk is conducted to find a weapon, does it actually find one?) Now we will use an outcome-style analysis. Specifically, we will fit a machine learning model to predict the probability that each stop which was conducted on suspicion the pedestrian possessed a weapon actually finds a weapon. Stops which are very unlikely to find a weapon arguably violate the Fourth Amendment, which prohibits unreasonable searches; if such stops disproportionately occur of certain race groups, the policy may violate the Fourteenth Amendment, which prohibits racial discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31133e",
   "metadata": {
    "id": "-CaMrTP0NlPY"
   },
   "source": [
    "a. In this portion of this analysis, you will be using a smaller version of the data to speed up model fitting. Load in \"small_sqf_sample\". As before, we need to do some data processing for consistency with previous of analysis of this data. Load in the data and filter for stops between 2008 and 2012 (including both 2008 and 2012); filter for stops of white, Black, and Hispanic pedestrians using the suspect_race column; and filter for stops conducted on suspicion of criminal posession of a weapon (ie, suspected_crime == 'cpw'). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c1ab9b",
   "metadata": {
    "id": "cDZV0_2ygaOM"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'small_sqf_sample.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5fb67c548f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"small_sqf_sample.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check: size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'small_sqf_sample.csv'"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "sample_data = pd.read_csv(\"small_sqf_sample.csv\")\n",
    "\n",
    "# check: size\n",
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a209970c",
   "metadata": {
    "id": "cDZV0_2ygaOM"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-034ecb3067d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# filtering for year between 2008 and 2012, renaming \"sample_data\" to \"sample_data_filtered\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_data_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2008\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2012\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check: print unique values in \"year\" columm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_data' is not defined"
     ]
    }
   ],
   "source": [
    "# filtering for year between 2008 and 2012, renaming \"sample_data\" to \"sample_data_filtered\" \n",
    "sample_data_filtered = sample_data.loc[(sample_data[\"year\"] >= 2008) & (sample_data[\"year\"]<=2012)]\n",
    "\n",
    "# check: print unique values in \"year\" columm\n",
    "print(sample_data_filtered[\"year\"].unique())\n",
    "\n",
    "# filtering for race\n",
    "selected_race = ['black', 'hispanic', 'white']\n",
    "sample_data_filtered = sample_data_filtered[sample_data_filtered[\"suspect_race\"].isin(selected_race)]\n",
    "\n",
    "# check: print unique values in \"suspect_race\" column\n",
    "print (sample_data_filtered[\"suspect_race\"].unique())\n",
    "\n",
    "# filtering for suspected crime\n",
    "sample_data_filtered = sample_data_filtered[sample_data_filtered[\"suspected_crime\"]==\"cpw\"]\n",
    "\n",
    "# check: print unique values in \"suspected_crime\" column\n",
    "print (sample_data_filtered[\"suspected_crime\"].unique())\n",
    "\n",
    "# check: print size\n",
    "sample_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f365578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>precinct</th>\n",
       "      <th>xcoord</th>\n",
       "      <th>ycoord</th>\n",
       "      <th>serial</th>\n",
       "      <th>radio_run</th>\n",
       "      <th>inside_outside</th>\n",
       "      <th>location_housing</th>\n",
       "      <th>...</th>\n",
       "      <th>suspect_height</th>\n",
       "      <th>suspect_weight</th>\n",
       "      <th>suspect_hair</th>\n",
       "      <th>suspect_eye</th>\n",
       "      <th>suspect_build</th>\n",
       "      <th>found_gun</th>\n",
       "      <th>found_weapon</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-06-26</td>\n",
       "      <td>09:35</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1011541.0</td>\n",
       "      <td>237353.0</td>\n",
       "      <td>3971.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>bald</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2412904</td>\n",
       "      <td>40.818113</td>\n",
       "      <td>-73.901402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008</td>\n",
       "      <td>2008-04-15</td>\n",
       "      <td>06:05</td>\n",
       "      <td>26.0</td>\n",
       "      <td>996993.0</td>\n",
       "      <td>236772.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>housing</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>130.0</td>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "      <td>thin</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1150567</td>\n",
       "      <td>40.816551</td>\n",
       "      <td>-73.953962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009-03-10</td>\n",
       "      <td>05:15</td>\n",
       "      <td>79.0</td>\n",
       "      <td>999104.0</td>\n",
       "      <td>192913.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>housing</td>\n",
       "      <td>...</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>180.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1651909</td>\n",
       "      <td>40.696166</td>\n",
       "      <td>-73.946433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01-18</td>\n",
       "      <td>18:55</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1033623.0</td>\n",
       "      <td>152712.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neither</td>\n",
       "      <td>...</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>270.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>heavy</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2732153</td>\n",
       "      <td>40.585698</td>\n",
       "      <td>-73.822243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-09-20</td>\n",
       "      <td>00:00</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1000691.0</td>\n",
       "      <td>240368.0</td>\n",
       "      <td>7546.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>housing</td>\n",
       "      <td>...</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2539768</td>\n",
       "      <td>40.826415</td>\n",
       "      <td>-73.940594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year        date   time  precinct     xcoord    ycoord  serial  radio_run  \\\n",
       "0   2010  2010-06-26  09:35      41.0  1011541.0  237353.0  3971.0       True   \n",
       "7   2008  2008-04-15  06:05      26.0   996993.0  236772.0  1310.0       True   \n",
       "17  2009  2009-03-10  05:15      79.0   999104.0  192913.0  3551.0      False   \n",
       "20  2011  2011-01-18  18:55     100.0  1033623.0  152712.0   108.0      False   \n",
       "23  2010  2010-09-20  00:00      32.0  1000691.0  240368.0  7546.0      False   \n",
       "\n",
       "    inside_outside location_housing  ...  suspect_height suspect_weight  \\\n",
       "0            False          neither  ...        6.000000          170.0   \n",
       "7            False          housing  ...        5.250000          130.0   \n",
       "17           False          housing  ...        5.583333          180.0   \n",
       "20           False          neither  ...        6.333333          270.0   \n",
       "23           False          housing  ...        5.500000          160.0   \n",
       "\n",
       "    suspect_hair suspect_eye  suspect_build  found_gun  found_weapon       id  \\\n",
       "0           bald       brown         medium      False         False  2412904   \n",
       "7          brown       brown           thin      False         False  1150567   \n",
       "17         black       brown         medium      False         False  1651909   \n",
       "20         black       brown          heavy      False         False  2732153   \n",
       "23         black       brown         medium      False         False  2539768   \n",
       "\n",
       "          lat        lon  \n",
       "0   40.818113 -73.901402  \n",
       "7   40.816551 -73.953962  \n",
       "17  40.696166 -73.946433  \n",
       "20  40.585698 -73.822243  \n",
       "23  40.826415 -73.940594  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check: display filtered dataset head\n",
    "sample_data_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08410065",
   "metadata": {
    "id": "confident-array"
   },
   "source": [
    "\n",
    "b. We will be fitting the regression model \n",
    "\n",
    "`found_weapon ~ C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex + ADDITIONAL_CIRCUMSTANCE_COLUMNS` \n",
    "\n",
    "where ADDITIONAL_CIRCUMSTANCE_COLUMNS are any columns that begin with \"stopped_bc\" or \"additional_\" besides \"additional_other\" and \"stopped_bc_other\". You can get these columns by running\n",
    "\n",
    "`ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in d.columns if ('stopped_bc' in a or 'additional_' in a) and a not in (['additional_other', 'stopped_bc_other'])]`\n",
    "\n",
    "You should have 18 additional columns. These columns provide more information about the circumstances of the stop, and we include them for consistency with the original analysis and because they turn out to be important for predictive performance. \n",
    "\n",
    "Drop any rows with missing values in any of the variables you need. \n",
    "\n",
    "Now, we need to put the data into a format which sklearn can use later - ie, numpy arrays. Do this with \"patsy\" library and the \"dmatrix\" function. You can call dmatrix as follows:\n",
    "\n",
    "`sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS),sqf_data, return_type='dataframe')`\n",
    "\n",
    "and it will return a dataframe on which you can fit a regression model. The first argument to dmatrix is the formula that you want to use to make the dataframe; the second argument gives patsy the data you want to use; return_type='dataframe' ensures that you get a dataframe, not a patsy object which is hard to use.\n",
    "\n",
    "Look at the output of dmatrix and explain what the columns mean. Why can't we just pass the columns from the original dataframe directly into the sklearn function? (8 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2091b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# create the additional columns (line breaks used for readibility)\n",
    "ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in sample_data_filtered.columns \n",
    "                                   if ('stopped_bc' in a or 'additional_' in a) \n",
    "                                   and a not in (['additional_other', 'stopped_bc_other'])]\n",
    "\n",
    "# check: length of additional columns → 18\n",
    "print(len(ADDITIONAL_CIRCUMSTANCE_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3fec886",
   "metadata": {
    "id": "n42o63nLgdET",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['found_weapon', 'precinct', 'suspect_race', 'location_housing', 'year', 'suspect_age', 'suspect_height', 'suspect_weight', 'suspect_sex', 'stopped_bc_object', 'stopped_bc_desc', 'stopped_bc_casing', 'stopped_bc_lookout', 'stopped_bc_clothing', 'stopped_bc_drugs', 'stopped_bc_furtive', 'stopped_bc_violent', 'stopped_bc_bulge', 'additional_report', 'additional_investigation', 'additional_proximity', 'additional_evasive', 'additional_associating', 'additional_direction', 'additional_highcrime', 'additional_time', 'additional_sights']\n"
     ]
    }
   ],
   "source": [
    "# set columns for model fitting; variables mentioned in regression model + additional cirumstances columns\n",
    "model_columns = [\"found_weapon\", \"precinct\", \"suspect_race\", \"location_housing\", \n",
    "                 \"year\", \"suspect_age\", \"suspect_height\", \"suspect_weight\", \"suspect_sex\"]\n",
    "\n",
    "model_columns = model_columns + ADDITIONAL_CIRCUMSTANCE_COLUMNS;\n",
    "\n",
    "# check: print column names\n",
    "print(model_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5e0f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# check: print amount columns\n",
    "print(len(model_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89850456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>found_weapon</th>\n",
       "      <th>precinct</th>\n",
       "      <th>suspect_race</th>\n",
       "      <th>location_housing</th>\n",
       "      <th>year</th>\n",
       "      <th>suspect_age</th>\n",
       "      <th>suspect_height</th>\n",
       "      <th>suspect_weight</th>\n",
       "      <th>suspect_sex</th>\n",
       "      <th>stopped_bc_object</th>\n",
       "      <th>...</th>\n",
       "      <th>stopped_bc_bulge</th>\n",
       "      <th>additional_report</th>\n",
       "      <th>additional_investigation</th>\n",
       "      <th>additional_proximity</th>\n",
       "      <th>additional_evasive</th>\n",
       "      <th>additional_associating</th>\n",
       "      <th>additional_direction</th>\n",
       "      <th>additional_highcrime</th>\n",
       "      <th>additional_time</th>\n",
       "      <th>additional_sights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>41.0</td>\n",
       "      <td>black</td>\n",
       "      <td>neither</td>\n",
       "      <td>2010</td>\n",
       "      <td>49</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>26.0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>housing</td>\n",
       "      <td>2008</td>\n",
       "      <td>35</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>130.0</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>79.0</td>\n",
       "      <td>black</td>\n",
       "      <td>housing</td>\n",
       "      <td>2009</td>\n",
       "      <td>30</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>180.0</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>black</td>\n",
       "      <td>neither</td>\n",
       "      <td>2011</td>\n",
       "      <td>31</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>270.0</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>32.0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>housing</td>\n",
       "      <td>2010</td>\n",
       "      <td>16</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   found_weapon  precinct suspect_race location_housing  year  suspect_age  \\\n",
       "0         False      41.0        black          neither  2010           49   \n",
       "1         False      26.0     hispanic          housing  2008           35   \n",
       "2         False      79.0        black          housing  2009           30   \n",
       "3         False     100.0        black          neither  2011           31   \n",
       "4         False      32.0     hispanic          housing  2010           16   \n",
       "\n",
       "   suspect_height  suspect_weight suspect_sex  stopped_bc_object  ...  \\\n",
       "0        6.000000           170.0        male              False  ...   \n",
       "1        5.250000           130.0      female              False  ...   \n",
       "2        5.583333           180.0        male              False  ...   \n",
       "3        6.333333           270.0        male              False  ...   \n",
       "4        5.500000           160.0        male              False  ...   \n",
       "\n",
       "   stopped_bc_bulge  additional_report  additional_investigation  \\\n",
       "0             False              False                     False   \n",
       "1             False              False                      True   \n",
       "2              True              False                     False   \n",
       "3             False               True                     False   \n",
       "4             False              False                     False   \n",
       "\n",
       "   additional_proximity  additional_evasive  additional_associating  \\\n",
       "0                 False               False                   False   \n",
       "1                  True               False                   False   \n",
       "2                 False               False                   False   \n",
       "3                 False               False                   False   \n",
       "4                 False               False                   False   \n",
       "\n",
       "   additional_direction  additional_highcrime  additional_time  \\\n",
       "0                 False                 False            False   \n",
       "1                 False                 False            False   \n",
       "2                 False                  True             True   \n",
       "3                 False                  True             True   \n",
       "4                 False                  True            False   \n",
       "\n",
       "   additional_sights  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter data with selected model columns\n",
    "sample_data_filtered = sample_data_filtered[model_columns]\n",
    "\n",
    "#reset index\n",
    "sample_data_filtered = sample_data_filtered.reset_index(drop=True)\n",
    "\n",
    "#check: display filtered dataset head with specific columns needed for the model\n",
    "sample_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71849ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: (36271, 27) size after drop: (36056, 27)\n"
     ]
    }
   ],
   "source": [
    "# drop missing values we need in dataset\n",
    "sample_data_dropped = sample_data_filtered.dropna()\n",
    "\n",
    "# check: compare initial size of data and after drop\n",
    "print(\"initial size:\", sample_data_filtered.shape, \"size after drop:\", sample_data_dropped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e981db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept\n",
      "C(precinct)[T.5.0]\n",
      "C(precinct)[T.6.0]\n",
      "C(precinct)[T.7.0]\n",
      "C(precinct)[T.9.0]\n",
      "C(precinct)[T.10.0]\n",
      "C(precinct)[T.13.0]\n",
      "C(precinct)[T.14.0]\n",
      "C(precinct)[T.17.0]\n",
      "C(precinct)[T.18.0]\n",
      "C(precinct)[T.19.0]\n",
      "C(precinct)[T.20.0]\n",
      "C(precinct)[T.22.0]\n",
      "C(precinct)[T.23.0]\n",
      "C(precinct)[T.24.0]\n",
      "C(precinct)[T.25.0]\n",
      "C(precinct)[T.26.0]\n",
      "C(precinct)[T.28.0]\n",
      "C(precinct)[T.30.0]\n",
      "C(precinct)[T.32.0]\n",
      "C(precinct)[T.33.0]\n",
      "C(precinct)[T.34.0]\n",
      "C(precinct)[T.40.0]\n",
      "C(precinct)[T.41.0]\n",
      "C(precinct)[T.42.0]\n",
      "C(precinct)[T.43.0]\n",
      "C(precinct)[T.44.0]\n",
      "C(precinct)[T.45.0]\n",
      "C(precinct)[T.46.0]\n",
      "C(precinct)[T.47.0]\n",
      "C(precinct)[T.48.0]\n",
      "C(precinct)[T.49.0]\n",
      "C(precinct)[T.50.0]\n",
      "C(precinct)[T.52.0]\n",
      "C(precinct)[T.60.0]\n",
      "C(precinct)[T.61.0]\n",
      "C(precinct)[T.62.0]\n",
      "C(precinct)[T.63.0]\n",
      "C(precinct)[T.66.0]\n",
      "C(precinct)[T.67.0]\n",
      "C(precinct)[T.68.0]\n",
      "C(precinct)[T.69.0]\n",
      "C(precinct)[T.70.0]\n",
      "C(precinct)[T.71.0]\n",
      "C(precinct)[T.72.0]\n",
      "C(precinct)[T.73.0]\n",
      "C(precinct)[T.75.0]\n",
      "C(precinct)[T.76.0]\n",
      "C(precinct)[T.77.0]\n",
      "C(precinct)[T.78.0]\n",
      "C(precinct)[T.79.0]\n",
      "C(precinct)[T.81.0]\n",
      "C(precinct)[T.83.0]\n",
      "C(precinct)[T.84.0]\n",
      "C(precinct)[T.88.0]\n",
      "C(precinct)[T.90.0]\n",
      "C(precinct)[T.94.0]\n",
      "C(precinct)[T.100.0]\n",
      "C(precinct)[T.101.0]\n",
      "C(precinct)[T.102.0]\n",
      "C(precinct)[T.103.0]\n",
      "C(precinct)[T.104.0]\n",
      "C(precinct)[T.105.0]\n",
      "C(precinct)[T.106.0]\n",
      "C(precinct)[T.107.0]\n",
      "C(precinct)[T.108.0]\n",
      "C(precinct)[T.109.0]\n",
      "C(precinct)[T.110.0]\n",
      "C(precinct)[T.111.0]\n",
      "C(precinct)[T.112.0]\n",
      "C(precinct)[T.113.0]\n",
      "C(precinct)[T.114.0]\n",
      "C(precinct)[T.115.0]\n",
      "C(precinct)[T.120.0]\n",
      "C(precinct)[T.122.0]\n",
      "C(precinct)[T.123.0]\n",
      "C(suspect_race)[T.hispanic]\n",
      "C(suspect_race)[T.white]\n",
      "C(location_housing)[T.neither]\n",
      "C(location_housing)[T.transit]\n",
      "C(year)[T.2009]\n",
      "C(year)[T.2010]\n",
      "C(year)[T.2011]\n",
      "C(year)[T.2012]\n",
      "suspect_sex[T.male]\n",
      "stopped_bc_object[T.True]\n",
      "stopped_bc_desc[T.True]\n",
      "stopped_bc_casing[T.True]\n",
      "stopped_bc_lookout[T.True]\n",
      "stopped_bc_clothing[T.True]\n",
      "stopped_bc_drugs[T.True]\n",
      "stopped_bc_furtive[T.True]\n",
      "stopped_bc_violent[T.True]\n",
      "stopped_bc_bulge[T.True]\n",
      "additional_report[T.True]\n",
      "additional_investigation[T.True]\n",
      "additional_proximity[T.True]\n",
      "additional_evasive[T.True]\n",
      "additional_associating[T.True]\n",
      "additional_direction[T.True]\n",
      "additional_highcrime[T.True]\n",
      "additional_time[T.True]\n",
      "additional_sights[T.True]\n",
      "C(precinct)[T.5.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.6.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.7.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.9.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.10.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.13.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.14.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.17.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.18.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.19.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.20.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.22.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.23.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.24.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.25.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.26.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.28.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.30.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.32.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.33.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.34.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.40.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.41.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.42.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.43.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.44.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.45.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.46.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.47.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.48.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.49.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.50.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.52.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.60.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.61.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.62.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.63.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.66.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.67.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.68.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.69.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.70.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.71.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.72.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.73.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.75.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.76.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.77.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.78.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.79.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.81.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.83.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.84.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.88.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.90.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.94.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.100.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.101.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.102.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.103.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.104.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.105.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.106.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.107.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.108.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.109.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.110.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.111.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.112.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.113.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.114.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.115.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.120.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.122.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.123.0]:C(suspect_race)[T.hispanic]\n",
      "C(precinct)[T.5.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.6.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.7.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.9.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.10.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.13.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.14.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.17.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.18.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.19.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.20.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.22.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.23.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.24.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.25.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.26.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.28.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.30.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.32.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.33.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.34.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.40.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.41.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.42.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.43.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.44.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.45.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.46.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.47.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.48.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.49.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.50.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.52.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.60.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.61.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.62.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.63.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.66.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.67.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.68.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.69.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.70.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.71.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.72.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.73.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.75.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.76.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.77.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.78.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.79.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.81.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.83.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.84.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.88.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.90.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.94.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.100.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.101.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.102.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.103.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.104.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.105.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.106.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.107.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.108.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.109.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.110.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.111.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.112.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.113.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.114.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.115.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.120.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.122.0]:C(suspect_race)[T.white]\n",
      "C(precinct)[T.123.0]:C(suspect_race)[T.white]\n",
      "suspect_age\n",
      "suspect_height\n",
      "suspect_weight\n"
     ]
    }
   ],
   "source": [
    "# import patsy and dmatrix\n",
    "import patsy\n",
    "from patsy import dmatrices, dmatrix, demo_data\n",
    "\n",
    "# set data in format sklearn can use with patsy and \n",
    "sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS), sample_data_dropped, return_type='dataframe')\n",
    "\n",
    "\n",
    "# check: see head dataframe converted to dmatrix\n",
    "#print(sqf_X.columns)\n",
    "\n",
    "for x in sqf_X.columns: print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213ace6",
   "metadata": {
    "id": "N3N-RXXKX1PQ"
   },
   "source": [
    "### Answer\n",
    "Look at the output of dmatrix and explain what the columns mean. Why can't we just pass the columns from the original dataframe directly into the sklearn function? (8 points)\n",
    "\n",
    "Each of the columns in the dmatrix is a unique value in a categorical variable. In other words, each row in the data does not have a descriptive value, instead it has a 0 for False and 1 for True. For example, a column named color has the unique values are red, green, blue. When tranforming this into a dmatrix, we create a column for each unique value and write a 1 in those columns if an entry corresponds to that color. This is important because, when running regression, values need to be numerical if they are being used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d160bb2",
   "metadata": {
    "id": "AAib9e-WfUdF"
   },
   "source": [
    "c. As discussed in class, when fitting machine learning models, you should always divide the dataset into a train, val, and test set. Randomly divide the filtered, processed data into three pieces - the train set (60%), the val set (20%) and the test set (20%). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "514f5667",
   "metadata": {
    "id": "ditQALf_geMN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 36056\n",
      "train: 21633 , train frac: 0.5999833592189927\n",
      "val: 7211 , val frac: 0.19999445307299757\n",
      "test: 7212 , test frac: 0.20002218770800975\n"
     ]
    }
   ],
   "source": [
    "# divide datasets in train, val and test sets.\n",
    "# source: https://stackoverflow.com/questions/17412439/how-to-split-data-into-trainset-and-testset-randomly\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split randomnly initial data into 60/40\n",
    "sqf_X_train, sqf_X_other = train_test_split(sqf_X, test_size=0.4)\n",
    "\n",
    "# split other data into 50/50, therefor 40 (sqf_X_other) goes 20/20\n",
    "sqf_X_val, sqf_X_test = train_test_split(sqf_X_other, test_size=0.5)\n",
    "\n",
    "# critique: above method of randomizing twice can raise questions of true randomization (probability) of a single entry ending up in a specific set\n",
    "\n",
    "# check: data lengths to verify 60/20/20\n",
    "\n",
    "print(\"total:\", len(sqf_X))\n",
    "print(\"train:\", len(sqf_X_train), \", train frac:\", len(sqf_X_train) / len(sqf_X))\n",
    "print(\"val:\", len(sqf_X_val), \", val frac:\", len(sqf_X_val) / len(sqf_X))\n",
    "print(\"test:\", len(sqf_X_test), \", test frac:\", len(sqf_X_test) / len(sqf_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ebeb1b3",
   "metadata": {
    "id": "FyVKwlbjgffV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get index values of all sets\n",
    "train_indexes = sqf_X_train.index.values\n",
    "val_indexes = sqf_X_val.index.values\n",
    "test_indexes = sqf_X_test.index.values\n",
    "\n",
    "# set y values of all sets\n",
    "train_y_weapon = sample_data_dropped[\"found_weapon\"].loc[train_indexes]\n",
    "val_y_weapon = sample_data_dropped[\"found_weapon\"].loc[val_indexes]\n",
    "test_y_weapon = sample_data_dropped[\"found_weapon\"].loc[test_indexes]\n",
    "\n",
    "\n",
    "# check values of x\n",
    "#print(\"train index values:\", sqf_X_train.index.values)\n",
    "#print(\"val index values:\", sqf_X_val.index.values)\n",
    "#print(\"test index values:\", sqf_X_test.index.values)\n",
    "\n",
    "# check values of y\n",
    "#print(\"train y values:\", list(train_y_weapon))\n",
    "#print(\"val y values:\", list(val_y_weapon))\n",
    "#print(\"test y values:\", list(test_y_weapon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23103d11",
   "metadata": {
    "id": "MWZp2Jw7TKVm"
   },
   "source": [
    "d. We will be training a regularized logistic regression model to predict the outcome. When using many machine learning models, including regularized logistic regression, it is important to preprocess the input features so they are all on the same scale, for reasons discussed in class. In this case, we will take each column in the input data, subtract its mean, and divide by its standard deviation. This makes it so each column of the data has mean 0 and standard deviation 1. \n",
    "\n",
    "When scaling the data, it is important to compute the scaling using only the train set, as shown in class. The reason is that we are pretending that the train data is all we have access to to fit our model fitting pipeline, so we cannot \"peek\" at the validation or test sets to generate our scaling. Use sklearn's StandardScaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to fit a scaling transform on the train set and transform the train set (you can use the fit_transform method on the train set).Then apply the fitted StandardScaler model to the validation and test sets as well (using the transform --- not the fit --- method). (Look at the notebook we went through in class on regularization and lasso if you are confused.) The transformed datasets are the final datasets you will feed into your logistic regression model. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4abf1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a scaling transform on the train set\n",
    "# source: lecture_7_regularization_and_lasso.ipynb by Emma Pierson \n",
    "# Use standardscaler to normalize the columns of our matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e5a28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform the trainset\n",
    "sqf_X_train = scaler.fit_transform(sqf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a748803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fitted standard scaler to validation set, using the transform method\n",
    "sqf_X_val = scaler.transform(sqf_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6b4f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fitted standard scaler to test set, using the transform method\n",
    "sqf_X_test = scaler.transform(sqf_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac8182",
   "metadata": {
    "id": "involved-implement"
   },
   "source": [
    "e. Uisng sklearn.linear_model.LogisticRegression, fit a model on the train set to predict found_weapon. Set the \"penalty\" argument to \"none\" so that the model will not use any regularization; this corresponds to fitting a regular logistic regression model. If you get a `ConvergenceWarning`, increase the number of iterations using the `max_iter` argument; this means the model optimization needs more iterations to converge. (Look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) if you are uncertain which arguments to use!)\n",
    "\n",
    "A standard measure of predictive performance for binary outcome variables like found_weapon is AUC. Higher values of AUC are better; an AUC of 1 means that the model is perfectly predicting the outcome; an AUC of 0.5 means that it is predicting it only as well as random chance. Report the AUC of the fitted model (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) on both the train and validation sets. What is the problem with using accuracy as a metric for this task? How does the train set AUC differ from the validation AUC, and does this make sense? (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dee78a0",
   "metadata": {
    "id": "ExHdN87ggimI"
   },
   "outputs": [],
   "source": [
    "# import logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# use logistic regression to fit a model on the train set to predit found_weapon\n",
    "lr = LogisticRegression(penalty=\"none\", max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7f7535a",
   "metadata": {
    "id": "ExHdN87ggimI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, penalty='none')\n"
     ]
    }
   ],
   "source": [
    "# fit lr on the training data\n",
    "model = lr.fit(sqf_X_train, train_y_weapon) \n",
    "\n",
    "# check: print (for succesful output signal)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbb37921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.21621970e-02 1.03737390e-02 4.34826845e-02 ... 2.65176355e-02\n",
      " 4.38225197e-02 2.06226211e-24]\n"
     ]
    }
   ],
   "source": [
    "# probabilty calculation found weapon false or true\n",
    "prob_weapon_train = lr.predict_proba(sqf_X_train)\n",
    "#print (prob_weapon_train)\n",
    "\n",
    "# split only to get true probability\n",
    "predic_true_weapon_train = prob_weapon_train[:, 1]\n",
    "\n",
    "# check: print values\n",
    "print(predic_true_weapon_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19d1ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8345741898698987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# train sets: give predic_true_weapon_train to roc function\n",
    "roc_auc_train = roc_auc_score(train_y_weapon, predic_true_weapon_train)\n",
    "\n",
    "print(roc_auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "311d9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8754695250150758\n"
     ]
    }
   ],
   "source": [
    "# predict found_weapon: validation set\n",
    "\n",
    "lr = LogisticRegression(penalty=\"none\", max_iter=1000)\n",
    "\n",
    "# 1. fit\n",
    "model = lr.fit(sqf_X_val, val_y_weapon) \n",
    "\n",
    "# 2. predict\n",
    "prob_weapon_train = lr.predict_proba(sqf_X_val)\n",
    "\n",
    "# 3. split \n",
    "predic_true_weapon_val = prob_weapon_train[:, 1]\n",
    "\n",
    "# 4. train\n",
    "roc_auc_val = roc_auc_score(val_y_weapon, predic_true_weapon_val)\n",
    "\n",
    "# 5. print\n",
    "print(roc_auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2ec0f",
   "metadata": {
    "id": "1Ep5iHbBhn1z"
   },
   "source": [
    "###  NEED ANSWER\n",
    "What is the problem with using accuracy as a metric for this task? How does the train set AUC differ from the validation AUC, and does this make sense? (7 points)\n",
    "\n",
    "For this task, we are trying to predict whether someone will have a weapon based off the information we gave the model. Using accuracy as a metric in this task is problematic because we know that this data is nto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8d6ac",
   "metadata": {
    "id": "eight-absorption"
   },
   "source": [
    "f. Our logistic regression model is not using any regularization and appears to be overfitting. We will try to use regularization to reduce overfitting. You will be fitting an L1-penalized logistic regression model using code that looks something like\n",
    "\n",
    "`LogisticRegression(C=sparsity_param, penalty='l1', solver='liblinear')`\n",
    "\n",
    "(the \"l1\" specifies that we're using an L1 penalty, as discussed in class; the \"liblinear\" solver is an optimizer that works with the L1 penalty. See the logistic regression documentation for more details.) \n",
    "\n",
    "Increase and decrease the amount of regularization using different values of the C parameter, searching logarithmically over at least 20 values in the range from 1e-2 to 1. (Note: we do not recommend defining a variable named \"C\" in your code, because this may cause weird patsy issues, since patsy also uses \"C\". Give the variable another name. Sorry! I complained to the patsy people.) \n",
    "\n",
    "Print out the train set, val set, and test set AUC for each value of of the regularization parameter. Make a plot where the x-axis is the regularization parameter and the y-axis is AUC, with one line for train AUC, one line for val AUC, and one line for test AUC (use plt.semilogx to plot the lines so the x-axis will be logarithmic, making it easier to see the plot). Comment on the trends. Do you see evidence of overfitting? Explain. For the rest of this assignment, use the model with the highest AUC on the validation set. (10 points)\n",
    "\n",
    "In a full analysis, it would make sense to play with other aspects of the model as well: for example, you could try using other forms of regularization (like L1 vs L2) or other classification algorithms besides logistic regression. The basic pattern, though, would be the same: fit the model on the train set, choose models on the val set, and once you've chosen your best model, assess your results (once!) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the lr\n",
    "lr = LogisticRegression(C=log_val, penalty='l1', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d15330c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.36563703  1.51650537  1.68404085  1.87008476  2.07668182  2.30610262\n",
      "  2.56086862  2.84377982  3.15794555  3.50681865  3.89423339  4.32444766\n",
      "  4.80218973  5.33271021  5.92183979  6.57605327  7.3025408   8.10928682\n",
      "  9.00515787 10.        ]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# WIP 🛶\n",
    "\n",
    "# 1. write func/list that ranges > 20 values from 1e-2 to 1\n",
    "log_values = np.logspace(start=math.e**(-2), stop=1, num=20)\n",
    "print (values)\n",
    "\n",
    "# 2. print out train, val and test set AUC for each val of the reg\n",
    "\n",
    "\n",
    "for value in log values\n",
    "    # 2.1 define lr with: LogisticRegression(C=sparsity_param, penalty='l1', solver='liblinear'\n",
    "    # 2.2 assign each value from 1. sparsity_param to C\n",
    "\n",
    "# 3. make plot \n",
    "    # 3.1 x-axis are values from 1. and logarithmic (plt.semilogx)\n",
    "    # 3.2 y-axis is AUC\n",
    "    # 3.3 plot lines\n",
    "        # 3.3.1 line for train AUC\n",
    "        # 3.3.1 line for val AUC\n",
    "        # 3.3.1 line for test AUC\n",
    "    # 3.4 add legenda\n",
    "    # 3.5 style\n",
    "    \n",
    "# 4. analyze trends (text comments: overfitting? explain! and chose highest AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59ccd1",
   "metadata": {
    "id": "owMXSAmTeInj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c22532",
   "metadata": {
    "id": "antique-connection"
   },
   "source": [
    "g. Assess model *calibration* on the test set. This checks whether the model's predicted probabilities line up with the true probabilities, and is important to assess here because we will be analyzing the model's predicted probabilities. To assess calibration, take the 10% of rows of the test set with the highest model predictions and compare the mean model predicted probability (ie, the output of `predict_proba`) to the actual mean of the outcome variable. Repeat for the next 10% of rows, and for all 10% groups. Make a plot comparing the model predicted probabilities on each 10% group to the actual outcomes (mean predicted probability on the x-axis and mean actual outcome on the y-axis). You should end up with a plot with 10 points, one for each 10% group. Plot the line y=x so you can see how well the predicted probabilities line up with the actual probabilities - ideally, your points will lie on the line or close to it. (10 points)\n",
    "\n",
    "Calibration is an issue for many machine learning models, including deep learning models, so this is always a good thing to check. In a full analysis, it would make sense to check calibration (and AUC) for subgroups as well (eg, each race group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24b4d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP 🛶\n",
    "\n",
    "# 1. write func(step (1,2,3 etc))\n",
    "    # 1.1 take 10% of rows in test set with highest model predictions \n",
    "    # 1.2 take mean model predicted (output of predict_proba)\n",
    "    # 1.3 compare 1. and 2. to the actual mean of the outcome variable (?where to get this?)\n",
    "\n",
    "# 2. loop over each 10% of rows and 10% groups (?what groups?) with func 1. \n",
    "\n",
    "# 3. make graph: \"model predicted probabilities VS actual outcomes\"\n",
    "    # 3.1 x-axis: mean predicted probability\n",
    "    # 3.2 y-axis: mean actual outcome\n",
    "    # 3.3 plot line: y=x (show: how well predicted prob line up with actual prob?)\n",
    "    # 3.4 style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f4dde",
   "metadata": {
    "id": "frozen-apollo"
   },
   "source": [
    "h. Using the test set, compute the fraction of observations which have lower than a 2% model-predicted probability of finding a weapon for Black, Hispanic, and white pedestrians. Stops below this threshold are extremely unlikely to have resulted in finding a weapon, arguably violating the Fourth Amendment. Repeat this for 20 thresholds evenly spaced between 1% and 5%. Make a graph where the x-axis is the threshold, and the y-axis is the fraction of stops falling below that threshold, with one line for each race group. Explain what you observe. Do you think this graph provides evidence for Fourteenth Amendment violations (racial discrimination)? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f2af541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP 🛶\n",
    "\n",
    "# 1. funct(prob_para (2%, 3% etc)). \n",
    "    # 1.1 compute frac of observations % model-predicted probability of finding a weapon\n",
    "    # 1.2 for Black\n",
    "    # 1.3 for Hispanic\n",
    "    # 1.4 for white\n",
    "    \n",
    "# 2. repeat 1. for 20 thresholds evenly spaced between 1% and 5%\n",
    "\n",
    "# 3. make graph\n",
    "    # 3.1 x-axis: threshold\n",
    "    # 3.2 x-axis: fraction of stops falling below that threshold\n",
    "    # 3.3 plot lines\n",
    "        # 3.3.1 for Black\n",
    "        # 3.3.2 for Hispanic\n",
    "        # 3.3.3 for white\n",
    "\n",
    "# 4. analyze/observe (comment: provide evidence 14th amendment violations? Or racial discrimintation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89b234",
   "metadata": {
    "id": "organized-luxembourg"
   },
   "source": [
    "i. The analysis you have performed in the second part of this assignment is very similar to the analysis in [this paper](https://5harad.com/papers/stop-and-frisk.pdf). Read the paper and write a few sentences about their main conclusions. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd197129",
   "metadata": {
    "id": "6wMU2Jzse3Dm"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
